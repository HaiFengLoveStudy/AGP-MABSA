# AGP-MABSA å¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ éªŒè¯ç¯å¢ƒ
```bash
python verify_setup.py
```

### 2ï¸âƒ£ å®‰è£…ä¾èµ–
```bash
conda create -n agp_mabsa python=3.9
conda activate agp_mabsa
pip install -r requirements.txt
```

### 3ï¸âƒ£ å‡†å¤‡æ•°æ®

**æ•°æ®æ ¼å¼ç¤ºä¾‹** (`data/raw/train.jsonl`):
```json
{"sample_id": "001", "text": "The food was great!", "aspect": "food", "image_paths": ["001.jpg"], "label": 2, "pair_id": "001"}
{"sample_id": "002", "text": "The food was great!", "aspect": "service", "image_paths": ["001.jpg"], "label": 1, "pair_id": "001"}
```

**ç›®å½•ç»“æ„**:
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ train.jsonl  â† åŸå§‹è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ dev.jsonl    â† åŸå§‹éªŒè¯æ•°æ®
â”‚   â””â”€â”€ test.jsonl   â† åŸå§‹æµ‹è¯•æ•°æ®
â””â”€â”€ images/
    â””â”€â”€ twitter2015_images/
        â”œâ”€â”€ 001.jpg
        â”œâ”€â”€ 002.jpg
        â””â”€â”€ ...
```

### 4ï¸âƒ£ LLMçŸ¥è¯†æ‰©å†™

**é…ç½®APIå¯†é’¥**:
```bash
export OPENAI_API_KEY="sk-..."
```

**è¿è¡Œæ‰©å†™**:
```bash
python src/data/llm_expansion.py
```

**è¾“å‡º**:
- `data/processed/train_expanded.jsonl`
- `data/processed/dev_expanded.jsonl`
- `data/processed/test_expanded.jsonl`

### 5ï¸âƒ£ è®­ç»ƒæ¨¡å‹

**ä¿®æ”¹é…ç½®** (å¯é€‰):
```bash
vim configs/training_config.yaml
```

**å¼€å§‹è®­ç»ƒ**:
```bash
# å‰å°è¿è¡Œ
python train.py

# åå°è¿è¡Œï¼ˆæ¨èï¼‰
nohup python train.py > logs/training_202601290215.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/training_202601290215.log
```

**è®­ç»ƒè¿‡ç¨‹**:
```
Epoch 1: Train Loss: 3.24 | Dev F1: 0.40
Epoch 2: Train Loss: 2.78 | Dev F1: 0.48
...
Epoch 15: Train Loss: 1.23 | Dev F1: 0.69
âœ… Best model saved!
```

### 6ï¸âƒ£ è¯„ä¼°æ¨¡å‹

```bash
python evaluate.py
```

**è¾“å‡ºç»“æœ**:
- æ§åˆ¶å°: è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
- `results/test_results.json`: JSONæ ¼å¼ç»“æœ
- `results/confusion_matrix.png`: æ··æ·†çŸ©é˜µå›¾

## ğŸ¯ é…ç½®è°ƒä¼˜

### å‡å°‘æ˜¾å­˜å ç”¨
```yaml
batch_size: 16      # ä»32é™åˆ°16
num_queries: 6      # ä»8é™åˆ°6
lora_rank: 4        # ä»8é™åˆ°4
```

### åŠ é€Ÿè®­ç»ƒ
```yaml
num_workers: 8      # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
use_amp: true       # ä½¿ç”¨æ··åˆç²¾åº¦
batch_size: 64      # å¢å¤§batchï¼ˆå¦‚æœæ˜¾å­˜è¶³å¤Ÿï¼‰
```

### æå‡æ€§èƒ½
```yaml
alpha: 1.5          # å¢å¼ºè·¨æ¨¡æ€å¯¹é½
beta: 0.8           # å¢å¼ºæƒ…æ„Ÿå¯¹æ¯”
gamma: 0.5          # å¢å¼ºæ–¹é¢è¯†åˆ«
lr_backbone: 2e-5   # æé«˜backboneå­¦ä¹ ç‡
```

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### æŸ¥çœ‹è®­ç»ƒå†å²
```python
import json
with open('models/checkpoints/training_history.json') as f:
    history = json.load(f)
print(f"Best Dev F1: {max(h['macro_f1'] for h in history['dev'])}")
```

### æ£€æŸ¥æ£€æŸ¥ç‚¹
```bash
ls -lh models/checkpoints/
# best_model.pt           - æœ€ä½³æ¨¡å‹
# checkpoint_epoch_5.pt   - ç¬¬5ä¸ªepoch
# checkpoint_epoch_10.pt  - ç¬¬10ä¸ªepoch
```

## ğŸ› å¸¸è§é—®é¢˜

### é—®é¢˜1: CUDA out of memory
**è§£å†³**: å‡å°batch_size
```yaml
batch_size: 16  # æˆ–æ›´å°
```

### é—®é¢˜2: Loss = NaN
**è§£å†³**: é™ä½å­¦ä¹ ç‡
```yaml
lr_backbone: 5e-6
lr_head: 5e-5
max_grad_norm: 0.5
```

### é—®é¢˜3: å›¾åƒåŠ è½½å¤±è´¥
**æ£€æŸ¥**: å›¾åƒè·¯å¾„æ˜¯å¦æ­£ç¡®
```bash
# åº”è¯¥èƒ½è®¿é—®åˆ°
ls data/images/twitter2015_images/001.jpg
```

### é—®é¢˜4: è¿‡æ‹Ÿåˆ
**è§£å†³**: å¢åŠ æ­£åˆ™åŒ–
```yaml
weight_decay: 0.05  # ä»0.01å¢åŠ 
# åœ¨æ¨¡å‹ä¸­å¢åŠ dropout
```

## ğŸ”¬ æµ‹è¯•å•ä¸ªæ¨¡å—

### æµ‹è¯•ç¼–ç å™¨
```bash
cd src/models
python encoders.py
```

### æµ‹è¯•æŸ¥è¯¢ç”Ÿæˆå™¨
```bash
python query_generator.py
```

### æµ‹è¯•å®Œæ•´æ¨¡å‹
```bash
python agp_model.py
```

### æµ‹è¯•æŸå¤±å‡½æ•°
```bash
cd ../losses
python total_loss.py
```

## ğŸ“ˆ å®éªŒæŠ€å·§

### 1. å°è§„æ¨¡æµ‹è¯•
å…ˆç”¨å°‘é‡æ•°æ®æµ‹è¯•ä»£ç :
```bash
head -100 data/raw/train.jsonl > data/raw/train_small.jsonl
# ä¿®æ”¹é…ç½®æŒ‡å‘train_small.jsonl
python train.py  # å¿«é€ŸéªŒè¯æµç¨‹
```

### 2. è°ƒè¯•æ¨¡å¼
åœ¨trainer.pyä¸­æ·»åŠ :
```python
if batch_idx == 0:  # åªè·‘ç¬¬ä¸€ä¸ªbatch
    break
```

### 3. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
åœ¨æ¨¡å‹è¾“å‡ºä¸­åŒ…å«äº†attention weights:
```python
outputs['text_attn_weights']  # [B, 9]
outputs['image_attn_weights']  # [B, 9]
```

## ğŸ“ è¿›é˜¶ä½¿ç”¨

### æ¢å¤è®­ç»ƒ
```python
# åœ¨train.pyä¸­æ·»åŠ 
checkpoint = torch.load('models/checkpoints/checkpoint_epoch_10.pt')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1
```

### æ¶ˆèå®éªŒ
åˆ›å»ºä¸åŒé…ç½®æ–‡ä»¶:
```bash
cp configs/training_config.yaml configs/ablation_no_supcon.yaml
# ä¿®æ”¹: beta: 0.0
python train.py --config configs/ablation_no_supcon.yaml
```

### æ¨ç†å•ä¸ªæ ·æœ¬
```python
from src.models.agp_model import AGPModel
model = AGPModel(...)
model.load_state_dict(torch.load('models/checkpoints/best_model.pt')['model_state_dict'])
model.eval()

# å‡†å¤‡å•ä¸ªæ ·æœ¬
output = model(batch)
pred = output['sentiment_logits'].argmax(dim=1)
```

## ğŸ“š å­¦ä¹ è·¯å¾„

**Day 1**: ç†è§£æ•°æ®æµ
- é˜…è¯» `dataset.py`
- è¿è¡Œ `create_dataloaders.py`

**Day 2**: ç†è§£æ¨¡å‹ç»“æ„
- é˜…è¯» `encoders.py`, `query_generator.py`
- è¿è¡Œå•å…ƒæµ‹è¯•

**Day 3**: ç†è§£æŸå¤±å‡½æ•°
- é˜…è¯» `total_loss.py`
- ç†è§£å¯¹æ¯”å­¦ä¹ æœºåˆ¶

**Day 4-5**: è¿è¡Œå®Œæ•´è®­ç»ƒ
- é…ç½®ç¯å¢ƒ
- è¿è¡Œè®­ç»ƒ
- åˆ†æç»“æœ

## ğŸ‰ æˆåŠŸæ ‡å¿—

âœ… `verify_setup.py` å…¨éƒ¨é€šè¿‡  
âœ… æ•°æ®åŠ è½½å™¨æˆåŠŸåˆ›å»º  
âœ… æ¨¡å‹å‰å‘ä¼ æ’­æ— é”™è¯¯  
âœ… è®­ç»ƒlossç¨³å®šä¸‹é™  
âœ… Dev F1 è¾¾åˆ° 0.66+  
âœ… æ··æ·†çŸ©é˜µåˆç†  

---

**éœ€è¦å¸®åŠ©?** æŸ¥çœ‹ `PROJECT_SUMMARY.md` å’Œ `AGP_EXPERIMENT_PROCEDURE.md`
