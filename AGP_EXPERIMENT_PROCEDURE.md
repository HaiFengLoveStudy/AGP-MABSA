# AGPå®éªŒæ‰§è¡Œæ­¥éª¤è¯¦è§£

**ç‰ˆæœ¬:** 1.0  
**åˆ›å»ºæ—¥æœŸ:** 2026-01-27  
**é€‚ç”¨åœºæ™¯:** å¤šæ¨¡æ€æ–¹é¢çº§æƒ…æ„Ÿåˆ†æï¼ˆMABSAï¼‰  
**é¢„è®¡å®Œæˆæ—¶é—´:** 10-12å°æ—¶

---

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£æ˜¯AGPæ–¹æ³•çš„**å®Œæ•´æ‰§è¡Œæ“ä½œæ‰‹å†Œ**ï¼Œæä¾›ä»ç¯å¢ƒæ­å»ºåˆ°æ¨¡å‹è¯„ä¼°çš„è¯¦ç»†æ­¥éª¤ã€‚æ¯ä¸ªæ­¥éª¤éƒ½åŒ…å«ï¼š
- ğŸ¯ ç›®æ ‡è¯´æ˜
- ğŸ“¥ è¾“å…¥è¦æ±‚
- âš™ï¸ æ‰§è¡Œå‘½ä»¤
- ğŸ“¤ é¢„æœŸè¾“å‡º
- âœ… éªŒè¯æ–¹æ³•

**ç›¸å…³æ–‡æ¡£ï¼š**
- `AGP METHOD GUIDE.md` - ç†è®ºè®¾è®¡å’Œæ–¹æ³•è®º
- `AGAA METHOD GUIDE.md` - å‚è€ƒå®ç°æ¨¡æ¿
- `CONTRASTIVE_LEARNING_ANALYSIS.md` - å¯¹æ¯”å­¦ä¹ åˆ†æå’Œæ”¹è¿›å»ºè®®

---

## ç›®å½•

1. [å®éªŒç¯å¢ƒå‡†å¤‡](#1-å®éªŒç¯å¢ƒå‡†å¤‡)
2. [æ•°æ®é¢„å¤„ç†](#2-æ•°æ®é¢„å¤„ç†)
3. [æ¨¡å‹æ¶æ„å®ç°](#3-æ¨¡å‹æ¶æ„å®ç°)
4. [æŸå¤±å‡½æ•°å®ç°](#4-æŸå¤±å‡½æ•°å®ç°)
5. [è®­ç»ƒæ‰§è¡Œæµç¨‹](#5-è®­ç»ƒæ‰§è¡Œæµç¨‹)
6. [æ¨¡å‹è¯„ä¼°ä¸åˆ†æ](#6-æ¨¡å‹è¯„ä¼°ä¸åˆ†æ)
7. [è°ƒè¯•ä¸ä¼˜åŒ–](#7-è°ƒè¯•ä¸ä¼˜åŒ–)

---

## 1. å®éªŒç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚



**æ¨èé…ç½®ï¼š**
- GPU: NVIDIA A100/A800 (80GBæ˜¾å­˜) 


**æ˜¾å­˜ä¼°ç®—ï¼š**
```
æ¨¡å‹å‚æ•°ï¼š
- BERT-base: 110M Ã— 4 bytes = 440MB
- ViT-base: 86M Ã— 4 bytes = 344MB
- æ–°å¢æ¨¡å—: ~50M Ã— 4 bytes = 200MB
- æ€»è®¡: ~1GB

è®­ç»ƒæ˜¾å­˜ï¼ˆBatch Size=32, FP16ï¼‰ï¼š
- æ¨¡å‹å‚æ•°: 1GB
- æ¢¯åº¦: 1GB
- ä¼˜åŒ–å™¨çŠ¶æ€: 2GB
- æ¿€æ´»å€¼: 8-12GB
- æ€»è®¡: ~15GB

å»ºè®®ï¼š24GBæ˜¾å­˜å¯è¿è¡Œï¼Œ40GBæ˜¾å­˜æ›´ç¨³å®š
```

### 1.2 è½¯ä»¶ç¯å¢ƒé…ç½®

**æ“ä½œç³»ç»Ÿï¼š**
- Ubuntu 
- Python 3.10.0
- PyTorch version: 2.9.1+cu128

**åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š**

```bash
# ä½¿ç”¨condaåˆ›å»ºç¯å¢ƒ
conda create -n agp_mabsa python=3.10
conda activate agp_mabsa



### 1.3 ä¾èµ–åº“å®‰è£…

**æ ¸å¿ƒä¾èµ–ï¼š**



å®‰è£…å‘½ä»¤ï¼š
```bash
pip install -r requirements.txt
```


### 1.4 é¡¹ç›®ç›®å½•ç»“æ„

**åˆ›å»ºç›®å½•ï¼š**

```bash
mkdir -p AGP-MABSA
cd AGP-MABSA

# åˆ›å»ºå­ç›®å½•
mkdir -p data/raw              # åŸå§‹æ•°æ®é›†
mkdir -p data/processed        # å¤„ç†åçš„æ•°æ®ï¼ˆå«LLMæ‰©å†™ï¼‰
mkdir -p data/images           # å›¾åƒæ–‡ä»¶
mkdir -p models/pretrained     # é¢„è®­ç»ƒæ¨¡å‹
mkdir -p models/checkpoints    # è®­ç»ƒæ£€æŸ¥ç‚¹
mkdir -p src                   # æºä»£ç 
mkdir -p logs                  # è®­ç»ƒæ—¥å¿—
mkdir -p results               # å®éªŒç»“æœ
mkdir -p configs               # é…ç½®æ–‡ä»¶
```

**å®Œæ•´ç›®å½•ç»“æ„ï¼š**

```
AGP-MABSA/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # åŸå§‹JSONLæ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ dev.jsonl
â”‚   â”‚   â””â”€â”€ test.jsonl
â”‚   â”œâ”€â”€ processed/             # LLMæ‰©å†™åçš„æ•°æ®
â”‚   â”‚   â”œâ”€â”€ train_expanded.jsonl
â”‚   â”‚   â”œâ”€â”€ dev_expanded.jsonl
â”‚   â”‚   â””â”€â”€ test_expanded.jsonl
â”‚   â””â”€â”€ images/                # Twitterå›¾åƒ
â”‚       â””â”€â”€ twitter2015_images/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pretrained/            # BERTå’ŒViTé¢„è®­ç»ƒæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ bert-base-uncased/
â”‚   â”‚   â””â”€â”€ vit-base-patch16-224/
â”‚   â””â”€â”€ checkpoints/           # è®­ç»ƒä¿å­˜çš„æ¨¡å‹
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â””â”€â”€ checkpoint_epoch_*.pt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py         # æ•°æ®é›†ç±»
â”‚   â”‚   â”œâ”€â”€ preprocess.py      # é¢„å¤„ç†è„šæœ¬
â”‚   â”‚   â””â”€â”€ llm_expansion.py   # LLMçŸ¥è¯†æ‰©å†™
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoders.py        # ç¼–ç å™¨
â”‚   â”‚   â”œâ”€â”€ query_generator.py # æ··åˆæŸ¥è¯¢ç”Ÿæˆå™¨
â”‚   â”‚   â”œâ”€â”€ attention.py       # æ³¨æ„åŠ›æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ pooling.py         # æ³¨æ„åŠ›æ± åŒ–
â”‚   â”‚   â””â”€â”€ agp_model.py       # å®Œæ•´æ¨¡å‹
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ classification.py  # åˆ†ç±»æŸå¤±
â”‚   â”‚   â”œâ”€â”€ infonce.py         # InfoNCEæŸå¤±
â”‚   â”‚   â””â”€â”€ supcon.py          # SupConæŸå¤±
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py         # è®­ç»ƒå™¨
â”‚   â”‚   â””â”€â”€ optimizer.py       # ä¼˜åŒ–å™¨é…ç½®
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ metrics.py         # è¯„ä¼°æŒ‡æ ‡
â”‚       â””â”€â”€ visualize.py       # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model_config.yaml      # æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ training_config.yaml   # è®­ç»ƒé…ç½®
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/           # TensorBoardæ—¥å¿—
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ predictions/           # é¢„æµ‹ç»“æœ
â”‚   â”œâ”€â”€ visualizations/        # å¯è§†åŒ–å›¾è¡¨
â”‚   â””â”€â”€ metrics/               # è¯„ä¼°æŒ‡æ ‡
â”œâ”€â”€ train.py                   # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py                # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md                  # é¡¹ç›®è¯´æ˜
```



**æ•°æ®æ ¼å¼æ£€æŸ¥ï¼š**

åŸå§‹JSONLæ ¼å¼ç¤ºä¾‹ï¼š
```json
{
  "sample_id": "twitter15_train_001",
  "text": "The steak was cold but the ambience was nice",
  "aspect": "food",
  "image_paths": ["twitter2015_images/001.jpg"],
  "label": 0,
  "pair_id": "twitter15_train_001"
}
```

**å­—æ®µè¯´æ˜ï¼š**
- `sample_id`: å”¯ä¸€æ ·æœ¬æ ‡è¯†ç¬¦
- `text`: è¯„è®ºæ–‡æœ¬
- `aspect`: ç›®æ ‡æ–¹é¢ï¼ˆfood/service/ambienceç­‰ï¼‰
- `image_paths`: å›¾åƒè·¯å¾„åˆ—è¡¨
- `label`: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆ0=è´Ÿé¢ï¼Œ1=ä¸­æ€§ï¼Œ2=æ­£é¢ï¼‰
- `pair_id`: å›¾æ–‡å¯¹æ ‡è¯†ï¼ˆåŒä¸€å›¾æ–‡å¯¹çš„ä¸åŒæ–¹é¢å…±äº«æ­¤IDï¼‰



## 2. æ•°æ®é¢„å¤„ç†

### 2.1 LLMç¦»çº¿çŸ¥è¯†æ‰©å†™

#### 2.1.1 è®¾è®¡åŸç†

**ç›®æ ‡ï¼š** å°†æŠ½è±¡çš„æ–¹é¢è¯ï¼ˆå¦‚"food"ï¼‰æ‰©å†™ä¸ºå…·ä½“çš„æè¿°æ€§çŸ­è¯­ï¼ˆå¦‚"taste presentation portion size and freshness of dishes"ï¼‰ï¼Œä¸ºæ¨¡å‹æä¾›æ›´ä¸°å¯Œçš„è¯­ä¹‰é”šç‚¹ã€‚

**çº¦æŸæ¡ä»¶ï¼š**
- æœ€å¤§10ä¸ªå•è¯
- ä½¿ç”¨ç®€å•ã€å£è¯­åŒ–çš„è‹±è¯­
- é€‚åˆç¤¾äº¤åª’ä½“è¯„è®ºåœºæ™¯
- æè¿°è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾

#### 2.1.2 Promptæ¨¡æ¿

```text
Role: You are an assistant for social media sentiment analysis.

Task: Expand the given aspect word into a short phrase describing its visual and textual features in a review context.

Constraint: Use simple, casual English. Maximum 10 words. No introductory filler.

Input Aspect: "food"
Output: "taste presentation portion size and freshness of dishes"

Input Aspect: "service"
Output: "waiter attitude serving speed and customer care quality"

Input Aspect: "{aspect_word}"
Output:
```

#### 2.1.3 å®ç°ä»£ç 

**é€‰é¡¹1ï¼šä½¿ç”¨OpenAI GPT-4o**

**æ¨¡å—è¯´æ˜ï¼š`src/data/llm_expansion.py`**
- **ç”¨é€”**: è°ƒç”¨ LLM å¯¹åŸå§‹ JSONL æ•°æ®ä¸­çš„æ–¹é¢è¯è¿›è¡Œæ‰¹é‡æ‰©å†™ï¼Œå°†æŠ½è±¡æ–¹é¢è½¬åŒ–ä¸ºå¯ç”¨äºæ£€ç´¢å’Œå¯¹æ¯”å­¦ä¹ çš„æè¿°æ€§çŸ­è¯­ã€‚
- **è¾“å…¥**: åŸå§‹æ•°æ® JSONL è·¯å¾„ï¼ˆå¦‚ `data/raw/train.jsonl`ï¼‰ã€ç›®æ ‡è¾“å‡º JSONL è·¯å¾„ã€å¯é€‰çš„ LLM æ¨¡å‹åç§°ã€‚
- **è¾“å‡º**: å¸¦æœ‰æ–°å¢å­—æ®µ `aspect_desc` çš„ JSONL æ–‡ä»¶ï¼ˆå¦‚ `data/processed/train_expanded.jsonl`ï¼‰ï¼Œä»¥åŠæ–¹é¢è¯åˆ°æ‰©å†™çŸ­è¯­çš„æ˜ å°„å­—å…¸ï¼ˆä»…åœ¨è„šæœ¬å†…éƒ¨ä½¿ç”¨ï¼‰ã€‚

```python
# src/data/llm_expansion.py
import json
import openai
from tqdm import tqdm
import time

# é…ç½®API
openai.api_key = "your-api-key-here"

PROMPT_TEMPLATE = """Role: You are an assistant for social media sentiment analysis.

Task: Expand the given aspect word into a short phrase describing its visual and textual features in a review context.

Constraint: Use simple, casual English. Maximum 10 words. No introductory filler.

Input Aspect: "food"
Output: "taste presentation portion size and freshness of dishes"

Input Aspect: "service"
Output: "waiter attitude serving speed and customer care quality"

Input Aspect: "{aspect_word}"
Output:"""

def expand_aspect_openai(aspect_word, model="gpt-4o"):
    """ä½¿ç”¨OpenAI APIæ‰©å†™æ–¹é¢è¯"""
    prompt = PROMPT_TEMPLATE.format(aspect_word=aspect_word)
    
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿æŒä¸€è‡´æ€§
            max_tokens=30
        )
        expansion = response.choices[0].message.content.strip()
        return expansion
    except Exception as e:
        print(f"APIè°ƒç”¨å¤±è´¥: {e}")
        return aspect_word  # å¤±è´¥æ—¶è¿”å›åŸè¯

def expand_dataset_openai(input_jsonl, output_jsonl):
    """æ‰¹é‡æ‰©å†™æ•°æ®é›†"""
    # è¯»å–æ•°æ®
    samples = []
    with open(input_jsonl, 'r', encoding='utf-8') as f:
        for line in f:
            samples.append(json.loads(line))
    
    # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„æ–¹é¢è¯
    unique_aspects = list(set([s['aspect'] for s in samples]))
    print(f"å‘ç° {len(unique_aspects)} ä¸ªå”¯ä¸€æ–¹é¢: {unique_aspects}")
    
    # æ‰¹é‡æ‰©å†™
    aspect_expansions = {}
    for aspect in tqdm(unique_aspects, desc="æ‰©å†™æ–¹é¢è¯"):
        expansion = expand_aspect_openai(aspect)
        aspect_expansions[aspect] = expansion
        print(f"  {aspect} -> {expansion}")
        time.sleep(0.5)  # é¿å…APIé™æµ
    
    # æ·»åŠ æ‰©å†™åˆ°æ¯ä¸ªæ ·æœ¬
    for sample in samples:
        sample['aspect_desc'] = aspect_expansions[sample['aspect']]
    
    # ä¿å­˜
    with open(output_jsonl, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"âœ… å®Œæˆï¼ä¿å­˜åˆ° {output_jsonl}")
    return aspect_expansions

if __name__ == '__main__':
    # æ‰©å†™è®­ç»ƒé›†
    expand_dataset_openai(
        'data/raw/train.jsonl',
        'data/processed/train_expanded.jsonl'
    )
    
    # æ‰©å†™éªŒè¯é›†
    expand_dataset_openai(
        'data/raw/dev.jsonl',
        'data/processed/dev_expanded.jsonl'
    )
    
    # æ‰©å†™æµ‹è¯•é›†
    expand_dataset_openai(
        'data/raw/test.jsonl',
        'data/processed/test_expanded.jsonl'
    )
```


#### 2.1.4 æ‰§è¡Œæ­¥éª¤

**æ­¥éª¤1ï¼šé…ç½®APIå¯†é’¥**

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
export OPENAI_API_KEY="sk-..."
# æˆ–
export DASHSCOPE_API_KEY="sk-..."
```

**æ­¥éª¤2ï¼šè¿è¡Œæ‰©å†™è„šæœ¬**

```bash
python src/data/llm_expansion.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
å‘ç° 3 ä¸ªå”¯ä¸€æ–¹é¢: ['food', 'service', 'ambience']
æ‰©å†™æ–¹é¢è¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.67s/it]
  food -> taste presentation portion size and freshness of dishes
  service -> waiter attitude serving speed and customer care quality
  ambience -> lighting decoration music atmosphere and seating comfort
âœ… å®Œæˆï¼ä¿å­˜åˆ° data/processed/train_expanded.jsonl
```

**æ­¥éª¤3ï¼šéªŒè¯æ‰©å†™ç»“æœ**

```python
# scripts/verify_expansion.py
import json

def verify_expansion(jsonl_path, num_samples=5):
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        samples = [json.loads(line) for line in f]
    
    print(f"éªŒè¯æ–‡ä»¶: {jsonl_path}")
    print(f"æ€»æ ·æœ¬æ•°: {len(samples)}")
    print(f"\nå‰{num_samples}ä¸ªæ ·æœ¬:")
    
    for i, sample in enumerate(samples[:num_samples]):
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"  åŸå§‹æ–¹é¢: {sample['aspect']}")
        print(f"  æ‰©å†™æè¿°: {sample.get('aspect_desc', 'NOT FOUND!')}")
        print(f"  æ–‡æœ¬: {sample['text'][:50]}...")
        print(f"  æ ‡ç­¾: {sample['label']}")

verify_expansion('data/processed/train_expanded.jsonl')
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] æ‰€æœ‰JSONLæ–‡ä»¶åŒ…å«`aspect_desc`å­—æ®µ
- [ ] æ‰©å†™æè¿°ä¸è¶…è¿‡10ä¸ªå•è¯
- [ ] æ‰©å†™æè¿°è¯­ä¹‰åˆç†ï¼Œä¸æ–¹é¢è¯ç›¸å…³
- [ ] æ²¡æœ‰APIé”™è¯¯æˆ–å¤±è´¥æ ·æœ¬

**é¢„è®¡è€—æ—¶ï¼š** 1-2å°æ—¶ï¼ˆå–å†³äºAPIé€Ÿåº¦å’Œæ•°æ®é›†å¤§å°ï¼‰

### 2.2 æ•°æ®åŠ è½½å™¨å®ç°

#### 2.2.1 æ•°æ®é›†ç±»

**æ¨¡å—è¯´æ˜ï¼š`src/data/dataset.py`**
- **ç”¨é€”**: ä»æ‰©å†™åçš„ JSONL æ–‡ä»¶å’Œå›¾åƒç›®å½•ä¸­è¯»å–æ ·æœ¬ï¼Œå®Œæˆæ–‡æœ¬ / æ–¹é¢æè¿°ç¼–ç å’Œå›¾åƒé¢„å¤„ç†ï¼Œç”Ÿæˆå¯ç›´æ¥ç”¨äºè®­ç»ƒçš„å•æ¡æ ·æœ¬ã€‚
- **è¾“å…¥**: JSONL è·¯å¾„ã€å›¾åƒæ ¹ç›®å½•ã€`BertTokenizer`ã€`ViTImageProcessor`ã€æ–‡æœ¬å’Œæ–¹é¢æè¿°çš„æœ€å¤§é•¿åº¦ç­‰é…ç½®å‚æ•°ã€‚
- **è¾“å‡º**: `__getitem__` è¿”å›åŒ…å«æ–‡æœ¬ tokenã€æ–¹é¢æè¿° tokenã€å›¾åƒå¼ é‡ã€æƒ…æ„Ÿæ ‡ç­¾ã€æ–¹é¢ IDã€pair_id ç›¸å…³ mask ç­‰é”®å€¼çš„å­—å…¸ï¼›`__len__` è¿”å›æ ·æœ¬æ•°é‡ã€‚

```python
# src/data/dataset.py
import json
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import BertTokenizer, ViTImageProcessor

class MABSADataset(Dataset):
    """å¤šæ¨¡æ€æ–¹é¢çº§æƒ…æ„Ÿåˆ†ææ•°æ®é›†"""
    
    def __init__(
        self,
        jsonl_path,
        image_root,
        tokenizer,
        image_processor,
        max_text_len=80,
        max_aspect_len=30
    ):
        """
        Args:
            jsonl_path: JSONLæ–‡ä»¶è·¯å¾„
            image_root: å›¾åƒæ ¹ç›®å½•
            tokenizer: BERT tokenizer
            image_processor: ViT image processor
            max_text_len: æ–‡æœ¬æœ€å¤§é•¿åº¦
            max_aspect_len: æ–¹é¢æè¿°æœ€å¤§é•¿åº¦
        """
        self.image_root = image_root
        self.tokenizer = tokenizer
        self.image_processor = image_processor
        self.max_text_len = max_text_len
        self.max_aspect_len = max_aspect_len
        
        # åŠ è½½æ•°æ®
        self.samples = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.samples.append(json.loads(line))
        
        # æ„å»ºæ–¹é¢è¯åˆ°IDçš„æ˜ å°„
        unique_aspects = sorted(list(set([s['aspect'] for s in self.samples])))
        self.aspect2id = {aspect: idx for idx, aspect in enumerate(unique_aspects)}
        self.id2aspect = {idx: aspect for aspect, idx in self.aspect2id.items()}
        self.num_aspects = len(unique_aspects)
        
        print(f"åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
        print(f"æ–¹é¢ç±»åˆ«: {unique_aspects}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. æ–‡æœ¬ç¼–ç 
        text_encoding = self.tokenizer(
            sample['text'],
            max_length=self.max_text_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # 2. æ–¹é¢æè¿°ç¼–ç 
        aspect_desc = sample.get('aspect_desc', sample['aspect'])
        aspect_encoding = self.tokenizer(
            aspect_desc,
            max_length=self.max_aspect_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # 3. å›¾åƒåŠ è½½å’Œé¢„å¤„ç†
        image_path = f"{self.image_root}/{sample['image_paths'][0]}"
        try:
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.image_processor(image, return_tensors='pt')['pixel_values']
        except Exception as e:
            print(f"å›¾åƒåŠ è½½å¤±è´¥: {image_path}, é”™è¯¯: {e}")
            # ä½¿ç”¨é»‘è‰²å›¾åƒä½œä¸ºå ä½ç¬¦
            image_tensor = torch.zeros(1, 3, 224, 224)
        
        # 4. æ ‡ç­¾å’Œå…ƒä¿¡æ¯
        label = sample['label']
        aspect_id = self.aspect2id[sample['aspect']]
        pair_id = sample['pair_id']
        
        return {
            'text_input_ids': text_encoding['input_ids'].squeeze(0),
            'text_attention_mask': text_encoding['attention_mask'].squeeze(0),
            'aspect_input_ids': aspect_encoding['input_ids'].squeeze(0),
            'aspect_attention_mask': aspect_encoding['attention_mask'].squeeze(0),
            'image': image_tensor.squeeze(0),
            'label': torch.tensor(label, dtype=torch.long),
            'aspect_id': torch.tensor(aspect_id, dtype=torch.long),
            'pair_id': pair_id,  # å­—ç¬¦ä¸²ï¼Œç”¨äºæ„å»ºpair_id_mask
            'sample_id': sample['sample_id']
        }

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
    # å †å å¼ é‡
    text_input_ids = torch.stack([item['text_input_ids'] for item in batch])
    text_attention_mask = torch.stack([item['text_attention_mask'] for item in batch])
    aspect_input_ids = torch.stack([item['aspect_input_ids'] for item in batch])
    aspect_attention_mask = torch.stack([item['aspect_attention_mask'] for item in batch])
    images = torch.stack([item['image'] for item in batch])
    labels = torch.stack([item['label'] for item in batch])
    aspect_ids = torch.stack([item['aspect_id'] for item in batch])
    
    # æ„å»ºpair_id_mask
    pair_ids = [item['pair_id'] for item in batch]
    batch_size = len(pair_ids)
    pair_id_mask = torch.zeros(batch_size, batch_size, dtype=torch.bool)
    for i in range(batch_size):
        for j in range(batch_size):
            if pair_ids[i] == pair_ids[j] and i != j:
                pair_id_mask[i, j] = True
    
    return {
        'text_input_ids': text_input_ids,
        'text_attention_mask': text_attention_mask,
        'aspect_input_ids': aspect_input_ids,
        'aspect_attention_mask': aspect_attention_mask,
        'images': images,
        'labels': labels,
        'aspect_ids': aspect_ids,
        'pair_id_mask': pair_id_mask,
        'sample_ids': [item['sample_id'] for item in batch]
    }
```

#### 2.2.2 åˆ›å»ºæ•°æ®åŠ è½½å™¨

**æ¨¡å—è¯´æ˜ï¼š`src/data/create_dataloaders.py`**
- **ç”¨é€”**: åŸºäºæ•°æ®é›†ç±» `MABSADataset` åˆ›å»ºè®­ç»ƒ / éªŒè¯ / æµ‹è¯•é›†çš„æ•°æ®åŠ è½½å™¨ï¼Œå¹¶ç»Ÿä¸€è¿”å›æ–¹é¢ç±»åˆ«æ•°ã€‚
- **è¾“å…¥**: è®­ç»ƒã€éªŒè¯ã€æµ‹è¯• JSONL è·¯å¾„ï¼Œå›¾åƒæ ¹ç›®å½•ï¼Œ`batch_size`ï¼Œ`num_workers` ç­‰æ•°æ®åŠ è½½é…ç½®ã€‚
- **è¾“å‡º**: `train_loader`ã€`dev_loader`ã€`test_loader` ä¸‰ä¸ª `DataLoader` å¯¹è±¡ï¼Œä»¥åŠ `num_aspects`ï¼ˆæ–¹é¢ç±»åˆ«æ•°é‡ï¼Œç”¨äºæ„å»ºæ¨¡å‹ï¼‰ã€‚

```python
# src/data/create_dataloaders.py
from torch.utils.data import DataLoader
from transformers import BertTokenizer, ViTImageProcessor
from dataset import MABSADataset, collate_fn

def create_dataloaders(
    train_jsonl='data/processed/train_expanded.jsonl',
    dev_jsonl='data/processed/dev_expanded.jsonl',
    test_jsonl='data/processed/test_expanded.jsonl',
    image_root='data/images',
    batch_size=32,
    num_workers=4
):
    """åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    
    # åˆå§‹åŒ–tokenizerå’Œimage processor
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MABSADataset(
        train_jsonl, image_root, tokenizer, image_processor
    )
    dev_dataset = MABSADataset(
        dev_jsonl, image_root, tokenizer, image_processor
    )
    test_dataset = MABSADataset(
        test_jsonl, image_root, tokenizer, image_processor
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    dev_loader = DataLoader(
        dev_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    return train_loader, dev_loader, test_loader, train_dataset.num_aspects

# æµ‹è¯•
if __name__ == '__main__':
    train_loader, dev_loader, test_loader, num_aspects = create_dataloaders(
        batch_size=8
    )
    
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(dev_loader)}")
    print(f"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
    print(f"æ–¹é¢ç±»åˆ«æ•°: {num_aspects}")
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    batch = next(iter(train_loader))
    print("\næ‰¹æ¬¡æ•°æ®å½¢çŠ¶:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        else:
            print(f"  {key}: {type(value)}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
cd src/data
python create_dataloaders.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
åŠ è½½ 4000 ä¸ªæ ·æœ¬
æ–¹é¢ç±»åˆ«: ['ambience', 'food', 'service']
åŠ è½½ 500 ä¸ªæ ·æœ¬
æ–¹é¢ç±»åˆ«: ['ambience', 'food', 'service']
åŠ è½½ 500 ä¸ªæ ·æœ¬
æ–¹é¢ç±»åˆ«: ['ambience', 'food', 'service']
è®­ç»ƒæ‰¹æ¬¡æ•°: 500
éªŒè¯æ‰¹æ¬¡æ•°: 63
æµ‹è¯•æ‰¹æ¬¡æ•°: 63
æ–¹é¢ç±»åˆ«æ•°: 3

æ‰¹æ¬¡æ•°æ®å½¢çŠ¶:
  text_input_ids: torch.Size([8, 80])
  text_attention_mask: torch.Size([8, 80])
  aspect_input_ids: torch.Size([8, 30])
  aspect_attention_mask: torch.Size([8, 30])
  images: torch.Size([8, 3, 224, 224])
  labels: torch.Size([8])
  aspect_ids: torch.Size([8])
  pair_id_mask: torch.Size([8, 8])
  sample_ids: <class 'list'>
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] æ•°æ®åŠ è½½å™¨æˆåŠŸåˆ›å»º
- [ ] æ‰¹æ¬¡æ•°æ®å½¢çŠ¶æ­£ç¡®
- [ ] pair_id_maskæ­£ç¡®æ„å»º
- [ ] å›¾åƒåŠ è½½æ— é”™è¯¯
- [ ] Tokenizationé•¿åº¦åˆé€‚

**é¢„è®¡è€—æ—¶ï¼š** 30åˆ†é’Ÿ

---

## 3. æ¨¡å‹æ¶æ„å®ç°

### 3.1 ç¼–ç å™¨é…ç½®

#### 3.1.1 æ–‡æœ¬ç¼–ç å™¨ï¼ˆBERTï¼‰

**è®¾è®¡ç­–ç•¥ï¼š**
- ä½¿ç”¨`bert-base-uncased`ï¼ˆ768ç»´ï¼Œ110Må‚æ•°ï¼‰
- **å†»ç»“ç­–ç•¥**ï¼šå‰10å±‚å†»ç»“ï¼Œåªå¾®è°ƒæœ€å2å±‚
- ç›®çš„ï¼šä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/models/encoders.py`**
- **ç”¨é€”**: å°è£…æ–‡æœ¬ç¼–ç å™¨ `TextEncoder`ï¼ˆåŸºäº BERTï¼Œæ”¯æŒéƒ¨åˆ†å†»ç»“ï¼‰å’Œå›¾åƒç¼–ç å™¨ `ImageEncoder`ï¼ˆåŸºäº ViTï¼Œæ”¯æŒ LoRA å¾®è°ƒï¼‰ï¼Œä¸ºåç»­æ¨¡å—æä¾›ç»Ÿä¸€çš„æ–‡æœ¬ / å›¾åƒç‰¹å¾ã€‚
- **è¾“å…¥**: æ–‡æœ¬ä¾§ä¸º `input_ids` å’Œ `attention_mask`ï¼ˆå½¢çŠ¶ `[B, L]`ï¼‰ï¼Œå›¾åƒä¾§ä¸º `pixel_values`ï¼ˆå½¢çŠ¶ `[B, 3, 224, 224]`ï¼‰ï¼Œä»¥åŠåˆå§‹åŒ–æ—¶çš„æ¨¡å‹åç§°ã€å†»ç»“å±‚æ•°ã€LoRA é…ç½®ç­‰ã€‚
- **è¾“å‡º**: æ–‡æœ¬ç¼–ç å™¨è¾“å‡º token çº§ç‰¹å¾ `[B, L, D]`ï¼Œå›¾åƒç¼–ç å™¨è¾“å‡º patch çº§ç‰¹å¾ `[B, P, D]`ï¼Œå…¶ä¸­ `D` ä¸ºéšè—ç»´åº¦ã€‚

```python
# src/models/encoders.py
import torch
import torch.nn as nn
from transformers import BertModel, ViTModel
from peft import LoraConfig, get_peft_model

class TextEncoder(nn.Module):
    """BERTæ–‡æœ¬ç¼–ç å™¨ï¼ˆéƒ¨åˆ†å†»ç»“ï¼‰"""
    
    def __init__(self, model_name='bert-base-uncased', freeze_layers=10):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.hidden_dim = self.bert.config.hidden_size  # 768
        
        # å†»ç»“å‰Nå±‚
        if freeze_layers > 0:
            # BERTæœ‰12å±‚ï¼ˆlayer 0-11ï¼‰
            for layer_idx in range(freeze_layers):
                for param in self.bert.encoder.layer[layer_idx].parameters():
                    param.requires_grad = False
            
            print(f"âœ… å†»ç»“BERTå‰{freeze_layers}å±‚ï¼Œå¾®è°ƒå{12-freeze_layers}å±‚")
    
    def forward(self, input_ids, attention_mask):
        """
        Args:
            input_ids: [B, L]
            attention_mask: [B, L]
        Returns:
            outputs: [B, L, D] tokençº§åˆ«çš„ç‰¹å¾
        """
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        return outputs.last_hidden_state  # [B, L, 768]

class ImageEncoder(nn.Module):
    """ViTå›¾åƒç¼–ç å™¨ï¼ˆLoRAå¾®è°ƒï¼‰"""
    
    def __init__(
        self,
        model_name='google/vit-base-patch16-224',
        use_lora=True,
        lora_rank=8,
        lora_alpha=16
    ):
        super().__init__()
        self.vit = ViTModel.from_pretrained(model_name)
        self.hidden_dim = self.vit.config.hidden_size  # 768
        
        if use_lora:
            # é…ç½®LoRA
            lora_config = LoraConfig(
                r=lora_rank,
                lora_alpha=lora_alpha,
                target_modules=["query", "value"],  # åªå¯¹Qå’ŒVæ³¨å…¥LoRA
                lora_dropout=0.1,
                bias="none"
            )
            
            # åº”ç”¨LoRA
            self.vit = get_peft_model(self.vit, lora_config)
            
            # æ‰“å°å¯è®­ç»ƒå‚æ•°
            trainable_params = sum(p.numel() for p in self.vit.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.vit.parameters())
            print(f"âœ… ViTåº”ç”¨LoRA (rank={lora_rank}, alpha={lora_alpha})")
            print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} "
                  f"({100*trainable_params/total_params:.2f}%)")
    
    def forward(self, pixel_values):
        """
        Args:
            pixel_values: [B, 3, 224, 224]
        Returns:
            outputs: [B, P, D] patchçº§åˆ«çš„ç‰¹å¾
        """
        outputs = self.vit(
            pixel_values=pixel_values,
            return_dict=True
        )
        # è¿”å›æ‰€æœ‰patchç‰¹å¾ï¼ˆä¸åŒ…æ‹¬CLS tokenï¼‰
        return outputs.last_hidden_state[:, 1:, :]  # [B, 196, 768]

# æµ‹è¯•ç¼–ç å™¨
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•æ–‡æœ¬ç¼–ç å™¨
    text_encoder = TextEncoder(freeze_layers=10).to(device)
    input_ids = torch.randint(0, 30000, (4, 80)).to(device)
    attention_mask = torch.ones(4, 80).to(device)
    text_features = text_encoder(input_ids, attention_mask)
    print(f"æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_features.shape}")  # [4, 80, 768]
    
    # æµ‹è¯•å›¾åƒç¼–ç å™¨
    image_encoder = ImageEncoder(use_lora=True, lora_rank=8).to(device)
    images = torch.randn(4, 3, 224, 224).to(device)
    image_features = image_encoder(images)
    print(f"å›¾åƒç‰¹å¾å½¢çŠ¶: {image_features.shape}")  # [4, 196, 768]
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
cd src/models
python encoders.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
âœ… å†»ç»“BERTå‰10å±‚,å¾®è°ƒå2å±‚
âœ… ViTåº”ç”¨LoRA (rank=8, alpha=16)
   å¯è®­ç»ƒå‚æ•°: 295,936 / 86,567,656 (0.34%)
æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: torch.Size([4, 80, 768])
å›¾åƒç‰¹å¾å½¢çŠ¶: torch.Size([4, 196, 768])
```

#### 3.1.2 æ··åˆæŸ¥è¯¢ç”Ÿæˆå™¨

**è®¾è®¡ç†å¿µï¼š**
- ç»“åˆ**éšå¼æŸ¥è¯¢**ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰å’Œ**æ˜¾å¼æŸ¥è¯¢**ï¼ˆLLMæ‰©å†™çš„æ–¹é¢æè¿°ï¼‰
- éšå¼æŸ¥è¯¢ï¼š8ä¸ªå¯å­¦ä¹ å‘é‡ï¼Œä»æ–¹é¢Embeddingåšæ®‹å·®å­¦ä¹ 
- æ˜¾å¼æŸ¥è¯¢ï¼šä½¿ç”¨BERTç¼–ç LLMæ‰©å†™çš„æè¿°ï¼Œå–[CLS]è¡¨ç¤º
- æœ€ç»ˆï¼š9ä¸ªæŸ¥è¯¢å‘é‡ï¼ˆ8éšå¼ + 1æ˜¾å¼ï¼‰

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/models/query_generator.py`**
- **ç”¨é€”**: å®ç°æ··åˆæŸ¥è¯¢ç”Ÿæˆå™¨ `HybridQueryGenerator`ï¼Œå°†æ–¹é¢ ID ä¸ LLM æ‰©å†™çš„æ–¹é¢æè¿°ç»“åˆï¼Œç”Ÿæˆéšå¼ + æ˜¾å¼çš„å¤šæŸ¥è¯¢å‘é‡ï¼Œç”¨äºåç»­äº¤å‰æ³¨æ„åŠ›ä»æ–‡æœ¬ / å›¾åƒä¸­æŠ½å–æ–¹é¢ç›¸å…³ä¿¡æ¯ã€‚
- **è¾“å…¥**: `aspect_ids`ï¼ˆå½¢çŠ¶ `[B]`ï¼‰ã€`aspect_desc_encoding`ï¼ˆå« `input_ids` å’Œ `attention_mask`ï¼‰ã€å…±äº«çš„ `TextEncoder` å®ä¾‹ã€‚
- **è¾“å‡º**: å½¢çŠ¶ä¸º `[B, 9, D]` çš„æŸ¥è¯¢å¼ é‡ï¼Œå…¶ä¸­å‰ 8 ä¸ªä¸ºéšå¼æŸ¥è¯¢ï¼Œæœ€å 1 ä¸ªä¸ºæ˜¾å¼æŸ¥è¯¢ã€‚

```python
# src/models/query_generator.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class HybridQueryGenerator(nn.Module):
    """æ··åˆæŸ¥è¯¢ç”Ÿæˆå™¨ï¼šéšå¼æŸ¥è¯¢ + æ˜¾å¼æŸ¥è¯¢"""
    
    def __init__(
        self,
        num_aspects,
        hidden_dim=768,
        num_learnable_queries=8
    ):
        """
        Args:
            num_aspects: æ–¹é¢ç±»åˆ«æ•°é‡
            hidden_dim: éšè—ç»´åº¦ï¼ˆ768ï¼‰
            num_learnable_queries: å¯å­¦ä¹ æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤8ï¼‰
        """
        super().__init__()
        self.num_aspects = num_aspects
        self.hidden_dim = hidden_dim
        self.num_learnable_queries = num_learnable_queries
        
        # æ–¹é¢Embeddingï¼ˆæ¯ä¸ªæ–¹é¢ä¸€ä¸ªåŸºç¡€å‘é‡ï¼‰
        self.aspect_embeddings = nn.Embedding(num_aspects, hidden_dim)
        
        # å¯å­¦ä¹ æŸ¥è¯¢å‚æ•°ï¼ˆå…±äº«ç»™æ‰€æœ‰æ–¹é¢ï¼‰
        self.learnable_params = nn.Parameter(
            torch.randn(num_learnable_queries, hidden_dim)
        )
        nn.init.xavier_uniform_(self.learnable_params)
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(hidden_dim)
    
    def forward(self, aspect_ids, aspect_desc_encoding, text_encoder):
        """
        Args:
            aspect_ids: [B] æ–¹é¢ID
            aspect_desc_encoding: dict with 'input_ids' [B, L] and 'attention_mask' [B, L]
            text_encoder: TextEncoderå®ä¾‹ï¼ˆå…±äº«BERTæƒé‡ï¼‰
        
        Returns:
            queries: [B, 9, D] æ··åˆæŸ¥è¯¢ï¼ˆ8éšå¼ + 1æ˜¾å¼ï¼‰
        """
        batch_size = aspect_ids.size(0)
        device = aspect_ids.device
        
        # === Part A: æ„é€ éšå¼æŸ¥è¯¢ ===
        # 1. è·å–æ–¹é¢åŸºç¡€å‘é‡ [B, D]
        base_aspect = self.aspect_embeddings(aspect_ids)
        
        # 2. å¹¿æ’­ç›¸åŠ ï¼š[B, 1, D] + [1, 8, D] -> [B, 8, D]
        implicit_queries = base_aspect.unsqueeze(1) + self.learnable_params.unsqueeze(0)
        
        # 3. å±‚å½’ä¸€åŒ–
        implicit_queries = self.layer_norm(implicit_queries)  # [B, 8, D]
        
        # === Part B: æ„é€ æ˜¾å¼æŸ¥è¯¢ ===
        # ä½¿ç”¨text_encoderçš„BERTç¼–ç LLMæè¿°
        desc_features = text_encoder(
            input_ids=aspect_desc_encoding['input_ids'],
            attention_mask=aspect_desc_encoding['attention_mask']
        )  # [B, L, D]
        
        # å–[CLS] tokenä½œä¸ºæ˜¾å¼æŸ¥è¯¢
        explicit_query = desc_features[:, 0, :].unsqueeze(1)  # [B, 1, D]
        
        # === Part C: æ‹¼æ¥ ===
        total_queries = torch.cat([implicit_queries, explicit_query], dim=1)  # [B, 9, D]
        
        return total_queries

# æµ‹è¯•
if __name__ == '__main__':
    from encoders import TextEncoder
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºç¼–ç å™¨å’ŒæŸ¥è¯¢ç”Ÿæˆå™¨
    text_encoder = TextEncoder().to(device)
    query_generator = HybridQueryGenerator(
        num_aspects=3,
        hidden_dim=768,
        num_learnable_queries=8
    ).to(device)
    
    # å‡†å¤‡è¾“å…¥
    batch_size = 4
    aspect_ids = torch.randint(0, 3, (batch_size,)).to(device)
    aspect_desc_encoding = {
        'input_ids': torch.randint(0, 30000, (batch_size, 30)).to(device),
        'attention_mask': torch.ones(batch_size, 30).to(device)
    }
    
    # ç”ŸæˆæŸ¥è¯¢
    queries = query_generator(aspect_ids, aspect_desc_encoding, text_encoder)
    print(f"æ··åˆæŸ¥è¯¢å½¢çŠ¶: {queries.shape}")  # [4, 9, 768]
    print(f"âœ… æŸ¥è¯¢ç”ŸæˆæˆåŠŸ: {8}ä¸ªéšå¼æŸ¥è¯¢ + {1}ä¸ªæ˜¾å¼æŸ¥è¯¢")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python query_generator.py
```

### 3.2 äº¤å‰æ³¨æ„åŠ›æ¨¡å—

**è®¾è®¡åŸç†ï¼š**
- ä½¿ç”¨æ–¹é¢æŸ¥è¯¢ä»æ–‡æœ¬å’Œå›¾åƒä¸­æå–ç›¸å…³ä¿¡æ¯
- æ ‡å‡†Transformerå—ï¼šå¤šå¤´äº¤å‰æ³¨æ„åŠ› + FFN + æ®‹å·®è¿æ¥

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/models/attention.py`**
- **ç”¨é€”**: å®šä¹‰æ–¹é¢å¼•å¯¼çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å— `AspectGuidedCrossAttention`ï¼Œè®©æ–¹é¢æŸ¥è¯¢ä»æ–‡æœ¬æˆ–å›¾åƒç‰¹å¾ä¸­é€‰æ‹©ä¸å½“å‰æ–¹é¢æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
- **è¾“å…¥**: `queries`ï¼ˆæ–¹é¢æŸ¥è¯¢ `[B, m, D]`ï¼‰ã€`keys` å’Œ `values`ï¼ˆæ–‡æœ¬æˆ–å›¾åƒç‰¹å¾ `[B, L, D]`ï¼‰ã€å¯é€‰çš„ `key_padding_mask`ã€‚
- **è¾“å‡º**: ç»è¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œåçš„æ–¹é¢ç‰¹å¾ `[B, m, D]`ï¼Œä½œä¸ºåç»­æ± åŒ–å’Œå¯¹æ¯”å­¦ä¹ çš„åŸºç¡€ã€‚

```python
# src/models/attention.py
import torch
import torch.nn as nn

class AspectGuidedCrossAttention(nn.Module):
    """æ–¹é¢å¼•å¯¼çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(
        self,
        hidden_dim=768,
        num_heads=8,
        dropout=0.1,
        feedforward_dim=2048
    ):
        super().__init__()
        
        # å¤šå¤´äº¤å‰æ³¨æ„åŠ›
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, feedforward_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feedforward_dim, hidden_dim),
            nn.Dropout(dropout)
        )
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, queries, keys, values, key_padding_mask=None):
        """
        Args:
            queries: [B, m, D] æ–¹é¢æŸ¥è¯¢
            keys: [B, L, D] æ–‡æœ¬/å›¾åƒç‰¹å¾
            values: [B, L, D] æ–‡æœ¬/å›¾åƒç‰¹å¾
            key_padding_mask: [B, L] padding mask (Trueè¡¨ç¤ºpaddingä½ç½®)
        
        Returns:
            output: [B, m, D] æå–çš„æ–¹é¢ç›¸å…³ç‰¹å¾
        """
        # äº¤å‰æ³¨æ„åŠ› + æ®‹å·®
        attn_output, _ = self.cross_attn(
            query=queries,
            key=keys,
            value=values,
            key_padding_mask=key_padding_mask,
            need_weights=False
        )
        queries = self.norm1(queries + self.dropout(attn_output))
        
        # FFN + æ®‹å·®
        ffn_output = self.ffn(queries)
        output = self.norm2(queries + ffn_output)
        
        return output

# æµ‹è¯•
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    cross_attn = AspectGuidedCrossAttention().to(device)
    
    # å‡†å¤‡è¾“å…¥
    queries = torch.randn(4, 9, 768).to(device)  # [B, m, D]
    text_features = torch.randn(4, 80, 768).to(device)  # [B, L, D]
    
    # äº¤å‰æ³¨æ„åŠ›
    output = cross_attn(queries, text_features, text_features)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [4, 9, 768]
    print(f"âœ… äº¤å‰æ³¨æ„åŠ›æ¨¡å—æ­£å¸¸å·¥ä½œ")
```

### 3.3 æ³¨æ„åŠ›æ± åŒ–

**è®¾è®¡åŸç†ï¼š**
- æ›¿ä»£ç®€å•çš„MeanPooling
- ä½¿ç”¨å¯å­¦ä¹ çš„èšåˆå‘é‡æ™ºèƒ½åŠ æƒå¤šä¸ªæŸ¥è¯¢
- é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹ é‡è¦æ€§

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/models/pooling.py`**
- **ç”¨é€”**: å®ç°æ³¨æ„åŠ›æ± åŒ–æ¨¡å— `AttentionPooling`ï¼Œä½¿ç”¨å¯å­¦ä¹ èšåˆå‘é‡å¯¹å¤šæŸ¥è¯¢ç‰¹å¾è¿›è¡ŒåŠ æƒæ±‡èšï¼Œå¾—åˆ°å•ä¸€çš„æ–¹é¢ç‰¹å¾è¡¨ç¤ºã€‚
- **è¾“å…¥**: å¤šæŸ¥è¯¢ç‰¹å¾ `Z`ï¼Œå½¢çŠ¶ `[B, m, D]`ã€‚
- **è¾“å‡º**: æ± åŒ–åçš„ç‰¹å¾ `pooled`ï¼ˆå½¢çŠ¶ `[B, D]`ï¼‰ä»¥åŠå¯¹åº”çš„æ³¨æ„åŠ›æƒé‡ï¼ˆå½¢çŠ¶ `[B, m]`ï¼Œç”¨äºè§£é‡Šæ¯ä¸ªæŸ¥è¯¢çš„é‡è¦æ€§ï¼‰ã€‚

```python
# src/models/pooling.py
import torch
import torch.nn as nn

class AttentionPooling(nn.Module):
    """æ³¨æ„åŠ›æ± åŒ–ï¼šæ™ºèƒ½èšåˆå¤šæŸ¥è¯¢ç‰¹å¾"""
    
    def __init__(self, hidden_dim=768, num_heads=4):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # å…¨å±€å¯å­¦ä¹ èšåˆå‘é‡
        self.aggregator = nn.Parameter(torch.randn(1, 1, hidden_dim))
        nn.init.xavier_uniform_(self.aggregator)
        
        # æ³¨æ„åŠ›æ¨¡å—
        self.attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
    
    def forward(self, Z):
        """
        Args:
            Z: [B, m, D] å¤šæŸ¥è¯¢ç‰¹å¾
        
        Returns:
            pooled: [B, D] èšåˆåçš„å•ä¸€ç‰¹å¾
        """
        batch_size = Z.size(0)
        
        # æ‰©å±•èšåˆå‘é‡åˆ°batch
        query = self.aggregator.expand(batch_size, -1, -1)  # [B, 1, D]
        
        # æ³¨æ„åŠ›æ± åŒ–ï¼šQ=aggregator, K=Z, V=Z
        output, attn_weights = self.attn(
            query=query,
            key=Z,
            value=Z,
            need_weights=True
        )  # output: [B, 1, D], attn_weights: [B, 1, m]
        
        pooled = output.squeeze(1)  # [B, D]
        
        return pooled, attn_weights.squeeze(1)  # [B, D], [B, m]

# æµ‹è¯•
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    pooling = AttentionPooling().to(device)
    
    # è¾“å…¥å¤šæŸ¥è¯¢ç‰¹å¾
    Z = torch.randn(4, 9, 768).to(device)
    
    # æ± åŒ–
    pooled, attn_weights = pooling(Z)
    print(f"æ± åŒ–å‰: {Z.shape}")  # [4, 9, 768]
    print(f"æ± åŒ–å: {pooled.shape}")  # [4, 768]
    print(f"æ³¨æ„åŠ›æƒé‡: {attn_weights.shape}")  # [4, 9]
    print(f"æƒé‡å’Œï¼ˆåº”çº¦ç­‰äº1ï¼‰: {attn_weights[0].sum().item():.4f}")
    print(f"âœ… æ³¨æ„åŠ›æ± åŒ–æ­£å¸¸å·¥ä½œ")
```

### 3.4 æŠ•å½±å¤´å’Œåˆ†ç±»å™¨

**æ¨¡å—è¯´æ˜ï¼š`src/models/projector.py`**
- **ç”¨é€”**: æä¾›å¯¹æ¯”å­¦ä¹ æ‰€éœ€çš„æŠ•å½±å¤´ `ProjectionHead`ï¼Œä»¥åŠæƒ…æ„Ÿåˆ†ç±»å™¨ `SentimentClassifier` å’Œæ–¹é¢åˆ†ç±»å™¨ `AspectClassifier`ï¼Œç»Ÿä¸€å®Œæˆç‰¹å¾æŠ•å½±å’Œåˆ†ç±»ä»»åŠ¡ã€‚
- **è¾“å…¥**: æŠ•å½±å¤´è¾“å…¥ä¸ºå•æ¨¡æ€ç‰¹å¾ `[B, D]`ï¼›æƒ…æ„Ÿåˆ†ç±»å™¨è¾“å…¥ä¸ºæ‹¼æ¥åçš„å¤šæ¨¡æ€ç‰¹å¾ `[B, 2D]`ï¼›æ–¹é¢åˆ†ç±»å™¨è¾“å…¥ä¸ºå•æ¨¡æ€ç‰¹å¾ `[B, D]`ã€‚
- **è¾“å‡º**: æŠ•å½±å¤´è¾“å‡º L2 å½’ä¸€åŒ–åçš„ç‰¹å¾ `[B, D']`ï¼›åˆ†ç±»å™¨è¾“å‡ºå¯¹åº”ç»´åº¦çš„ logitsï¼ˆæƒ…æ„Ÿ `[B, 3]`ï¼Œæ–¹é¢ `[B, num_aspects]`ï¼‰ã€‚

```python
# src/models/projector.py
import torch
import torch.nn as nn

class ProjectionHead(nn.Module):
    """æŠ•å½±å¤´ï¼šç”¨äºå¯¹æ¯”å­¦ä¹ """
    
    def __init__(self, input_dim=768, proj_dim=256, dropout=0.1):
        super().__init__()
        self.projection = nn.Sequential(
            nn.Linear(input_dim, proj_dim),
            nn.BatchNorm1d(proj_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(proj_dim, proj_dim)
        )
    
    def forward(self, x):
        """
        Args:
            x: [B, D] è¾“å…¥ç‰¹å¾
        Returns:
            h: [B, D'] L2å½’ä¸€åŒ–çš„æŠ•å½±ç‰¹å¾
        """
        h = self.projection(x)
        h = nn.functional.normalize(h, p=2, dim=1)  # L2å½’ä¸€åŒ–
        return h

class SentimentClassifier(nn.Module):
    """æƒ…æ„Ÿåˆ†ç±»å™¨"""
    
    def __init__(self, input_dim=1536, hidden_dim=512, num_classes=3, dropout=0.1):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
    
    def forward(self, x):
        """
        Args:
            x: [B, 2*D] æ‹¼æ¥çš„å¤šæ¨¡æ€ç‰¹å¾
        Returns:
            logits: [B, 3] æƒ…æ„Ÿé¢„æµ‹logits
        """
        return self.classifier(x)

class AspectClassifier(nn.Module):
    """æ–¹é¢åˆ†ç±»å™¨ï¼ˆè¾…åŠ©ä»»åŠ¡ï¼‰"""
    
    def __init__(self, input_dim=768, num_aspects=3, dropout=0.1):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, num_aspects)
        )
    
    def forward(self, x):
        """
        Args:
            x: [B, D] å•æ¨¡æ€ç‰¹å¾
        Returns:
            logits: [B, num_aspects] æ–¹é¢é¢„æµ‹logits
        """
        return self.classifier(x)
```

### 3.5 å®Œæ•´æ¨¡å‹

**æ¨¡å—è¯´æ˜ï¼š`src/models/agp_model.py`**
- **ç”¨é€”**: å°†ç¼–ç å™¨ã€æŸ¥è¯¢ç”Ÿæˆå™¨ã€äº¤å‰æ³¨æ„åŠ›ã€æ³¨æ„åŠ›æ± åŒ–ã€æŠ•å½±å¤´å’Œåˆ†ç±»å™¨æ•´åˆä¸ºå®Œæ•´çš„ `AGPModel`ï¼Œç»Ÿä¸€å®ç°å‰å‘ä¼ æ’­å’Œå¤šä»»åŠ¡è¾“å‡ºã€‚
- **è¾“å…¥**: ä¸€ä¸ª batch çš„æ•°æ®å­—å…¸ï¼ˆé€šå¸¸æ¥è‡ª `DataLoader`ï¼‰ï¼ŒåŒ…å«æ–‡æœ¬ tokenã€æ–¹é¢æè¿° tokenã€å›¾åƒå¼ é‡ã€æƒ…æ„Ÿæ ‡ç­¾ã€æ–¹é¢ IDã€pair_id mask ç­‰ä¿¡æ¯ã€‚
- **è¾“å‡º**: åŒ…å«å¤šç§ä¸­é—´ç»“æœå’Œæœ€ç»ˆç»“æœçš„å­—å…¸ï¼Œä¾‹å¦‚ `h_text`ã€`h_image`ã€`sentiment_logits`ã€`aspect_logits_text`ã€`aspect_logits_image` ç­‰ï¼Œç”¨äºè®¡ç®—è”åˆæŸå¤±å’Œè¯„ä¼°ã€‚

```python
# src/models/agp_model.py
import torch
import torch.nn as nn
from encoders import TextEncoder, ImageEncoder
from query_generator import HybridQueryGenerator
from attention import AspectGuidedCrossAttention
from pooling import AttentionPooling
from projector import ProjectionHead, SentimentClassifier, AspectClassifier

class AGPModel(nn.Module):
    """å®Œæ•´çš„AGPæ¨¡å‹"""
    
    def __init__(
        self,
        num_aspects,
        hidden_dim=768,
        proj_dim=256,
        num_queries=8,
        num_classes=3,
        freeze_bert_layers=10,
        use_lora=True,
        lora_rank=8
    ):
        super().__init__()
        
        # ç¼–ç å™¨
        self.text_encoder = TextEncoder(freeze_layers=freeze_bert_layers)
        self.image_encoder = ImageEncoder(use_lora=use_lora, lora_rank=lora_rank)
        
        # æŸ¥è¯¢ç”Ÿæˆå™¨
        self.query_generator = HybridQueryGenerator(
            num_aspects=num_aspects,
            hidden_dim=hidden_dim,
            num_learnable_queries=num_queries
        )
        
        # äº¤å‰æ³¨æ„åŠ›
        self.text_cross_attn = AspectGuidedCrossAttention(hidden_dim=hidden_dim)
        self.image_cross_attn = AspectGuidedCrossAttention(hidden_dim=hidden_dim)
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.text_pooling = AttentionPooling(hidden_dim=hidden_dim)
        self.image_pooling = AttentionPooling(hidden_dim=hidden_dim)
        
        # æŠ•å½±å¤´ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        self.text_proj = ProjectionHead(hidden_dim, proj_dim)
        self.image_proj = ProjectionHead(hidden_dim, proj_dim)
        
        # åˆ†ç±»å™¨
        self.sentiment_classifier = SentimentClassifier(
            input_dim=hidden_dim * 2,
            num_classes=num_classes
        )
        
        # è¾…åŠ©ä»»åŠ¡ï¼šæ–¹é¢åˆ†ç±»å™¨
        self.aspect_classifier_text = AspectClassifier(hidden_dim, num_aspects)
        self.aspect_classifier_image = AspectClassifier(hidden_dim, num_aspects)
    
    def forward(self, batch):
        """
        Args:
            batch: dictåŒ…å«æ‰€æœ‰è¾“å…¥
        
        Returns:
            dictåŒ…å«æ‰€æœ‰è¾“å‡º
        """
        # 1. ç¼–ç æ–‡æœ¬å’Œå›¾åƒ
        text_features = self.text_encoder(
            batch['text_input_ids'],
            batch['text_attention_mask']
        )  # [B, L, D]
        
        image_features = self.image_encoder(batch['images'])  # [B, P, D]
        
        # 2. ç”Ÿæˆæ··åˆæŸ¥è¯¢
        aspect_desc_encoding = {
            'input_ids': batch['aspect_input_ids'],
            'attention_mask': batch['aspect_attention_mask']
        }
        queries = self.query_generator(
            batch['aspect_ids'],
            aspect_desc_encoding,
            self.text_encoder
        )  # [B, m, D]
        
        # 3. äº¤å‰æ³¨æ„åŠ›æå–æ–¹é¢ç›¸å…³ç‰¹å¾
        # æ³¨æ„ï¼šéœ€è¦åè½¬attention_maskï¼ˆ1->False, 0->Trueï¼‰
        text_padding_mask = (batch['text_attention_mask'] == 0)
        
        Z_text = self.text_cross_attn(
            queries=queries,
            keys=text_features,
            values=text_features,
            key_padding_mask=text_padding_mask
        )  # [B, m, D]
        
        Z_image = self.image_cross_attn(
            queries=queries,
            keys=image_features,
            values=image_features
        )  # [B, m, D]
        
        # 4. æ³¨æ„åŠ›æ± åŒ–
        g_text, text_attn_weights = self.text_pooling(Z_text)  # [B, D]
        g_image, image_attn_weights = self.image_pooling(Z_image)  # [B, D]
        
        # 5. æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
        h_text = self.text_proj(g_text)  # [B, D']
        h_image = self.image_proj(g_image)  # [B, D']
        
        # 6. æ‹¼æ¥å¤šæ¨¡æ€ç‰¹å¾
        multimodal_feature = torch.cat([g_text, g_image], dim=1)  # [B, 2D]
        
        # 7. æƒ…æ„Ÿåˆ†ç±»
        sentiment_logits = self.sentiment_classifier(multimodal_feature)  # [B, 3]
        
        # 8. è¾…åŠ©ä»»åŠ¡ï¼šæ–¹é¢åˆ†ç±»
        aspect_logits_text = self.aspect_classifier_text(g_text)
        aspect_logits_image = self.aspect_classifier_image(g_image)
        
        return {
            'sentiment_logits': sentiment_logits,
            'aspect_logits_text': aspect_logits_text,
            'aspect_logits_image': aspect_logits_image,
            'h_text': h_text,
            'h_image': h_image,
            'g_text': g_text,
            'g_image': g_image,
            'Z_text': Z_text,
            'Z_image': Z_image,
            'text_attn_weights': text_attn_weights,
            'image_attn_weights': image_attn_weights
        }

# æµ‹è¯•å®Œæ•´æ¨¡å‹
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = AGPModel(
        num_aspects=3,
        num_queries=8
    ).to(device)
    
    # æ¨¡æ‹Ÿä¸€ä¸ªbatch
    batch = {
        'text_input_ids': torch.randint(0, 30000, (4, 80)).to(device),
        'text_attention_mask': torch.ones(4, 80).to(device),
        'aspect_input_ids': torch.randint(0, 30000, (4, 30)).to(device),
        'aspect_attention_mask': torch.ones(4, 30).to(device),
        'images': torch.randn(4, 3, 224, 224).to(device),
        'aspect_ids': torch.randint(0, 3, (4,)).to(device)
    }
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(batch)
    
    print("=== æ¨¡å‹è¾“å‡º ===")
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"{key}: {value.shape}")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n=== å‚æ•°ç»Ÿè®¡ ===")
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
    print(f"âœ… æ¨¡å‹æ„å»ºæˆåŠŸï¼")
```

**è¿è¡Œå®Œæ•´æµ‹è¯•ï¼š**
```bash
cd src/models
python agp_model.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
âœ… å†»ç»“BERTå‰10å±‚,å¾®è°ƒå2å±‚
âœ… ViTåº”ç”¨LoRA (rank=8, alpha=16)
   å¯è®­ç»ƒå‚æ•°: 295,936 / 86,567,656 (0.34%)

=== æ¨¡å‹è¾“å‡º ===
sentiment_logits: torch.Size([4, 3])
aspect_logits_text: torch.Size([4, 3])
aspect_logits_image: torch.Size([4, 3])
h_text: torch.Size([4, 256])
h_image: torch.Size([4, 256])
g_text: torch.Size([4, 768])
g_image: torch.Size([4, 768])
Z_text: torch.Size([4, 9, 768])
Z_image: torch.Size([4, 9, 768])
text_attn_weights: torch.Size([4, 9])
image_attn_weights: torch.Size([4, 9])

=== å‚æ•°ç»Ÿè®¡ ===
æ€»å‚æ•°: 201,234,567
å¯è®­ç»ƒå‚æ•°: 25,678,123 (12.76%)
âœ… æ¨¡å‹æ„å»ºæˆåŠŸï¼
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] æ‰€æœ‰æ¨¡å—å•ç‹¬æµ‹è¯•é€šè¿‡
- [ ] å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸ
- [ ] è¾“å‡ºå½¢çŠ¶ç¬¦åˆé¢„æœŸ
- [ ] å‚æ•°å†»ç»“ç­–ç•¥æ­£ç¡®
- [ ] LoRAæ³¨å…¥æˆåŠŸ

**é¢„è®¡è€—æ—¶ï¼š** 4-6å°æ—¶

---

## 4. æŸå¤±å‡½æ•°å®ç°

### 4.1 æŸå¤±å‡½æ•°ä½“ç³»æ¦‚è§ˆ

**æ€»æŸå¤±å‡½æ•°ï¼š**

```
L_total = L_cls + Î±Â·L_InfoNCE + Î²Â·L_SupCon + Î³Â·L_aux

å…¶ä¸­ï¼š
- L_cls: æƒ…æ„Ÿåˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
- L_InfoNCE: è·¨æ¨¡æ€å¯¹é½æŸå¤±
- L_SupCon: Aspect-Awareæƒ…æ„Ÿå¯åˆ†ç¦»æŸå¤±
- L_aux: è¾…åŠ©ä»»åŠ¡æŸå¤±ï¼ˆæ–¹é¢åˆ†ç±»ï¼‰
- Î±, Î², Î³: æŸå¤±æƒé‡ï¼ˆæ¨èï¼šÎ±=1.0, Î²=0.5, Î³=0.3ï¼‰
```

### 4.2 è¾…åŠ©ä»»åŠ¡æŸå¤±ï¼ˆæ–¹é¢åˆ†ç±»ï¼‰

**ç›®æ ‡ï¼š** ç¡®ä¿æå–çš„ç‰¹å¾åŒ…å«æ–¹é¢ä¿¡æ¯ï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ æ–¹é¢å¯¼å‘çš„è¡¨ç¤ºã€‚

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/losses/auxiliary.py`**
- **ç”¨é€”**: å®ç°è¾…åŠ©ä»»åŠ¡æŸå¤± `AuxiliaryAspectLoss`ï¼Œé€šè¿‡åŒæ—¶çº¦æŸæ–‡æœ¬å’Œå›¾åƒçš„æ–¹é¢é¢„æµ‹ï¼Œä½¿æ¨¡å‹æ˜¾å¼å­¦ä¹ æ–¹é¢ä¿¡æ¯ã€‚
- **è¾“å…¥**: æ–‡æœ¬æ–¹é¢ logits `aspect_logits_text`ã€å›¾åƒæ–¹é¢ logits `aspect_logits_image`ï¼ˆå½¢çŠ¶å‡ä¸º `[B, num_aspects]`ï¼‰ï¼Œä»¥åŠçœŸå®æ–¹é¢æ ‡ç­¾ `aspect_ids`ï¼ˆ`[B]`ï¼‰ã€‚
- **è¾“å‡º**: æ ‡é‡æŸå¤±å€¼ `loss`ï¼Œä»¥åŠåŒ…å«æ–‡æœ¬ / å›¾åƒåˆ†æ”¯å•ç‹¬æŸå¤±çš„å­—å…¸ï¼ˆç”¨äºæ—¥å¿—å’Œåˆ†æï¼‰ã€‚

```python
# src/losses/auxiliary.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class AuxiliaryAspectLoss(nn.Module):
    """è¾…åŠ©ä»»åŠ¡ï¼šæ–¹é¢åˆ†ç±»æŸå¤±"""
    
    def __init__(self):
        super().__init__()
        self.criterion = nn.CrossEntropyLoss()
    
    def forward(self, aspect_logits_text, aspect_logits_image, aspect_ids):
        """
        Args:
            aspect_logits_text: [B, num_aspects] æ–‡æœ¬æ–¹é¢é¢„æµ‹
            aspect_logits_image: [B, num_aspects] å›¾åƒæ–¹é¢é¢„æµ‹
            aspect_ids: [B] çœŸå®æ–¹é¢æ ‡ç­¾
        
        Returns:
            loss: scalar
        """
        loss_text = self.criterion(aspect_logits_text, aspect_ids)
        loss_image = self.criterion(aspect_logits_image, aspect_ids)
        
        # å–å¹³å‡
        loss = (loss_text + loss_image) / 2
        
        return loss, {'loss_text': loss_text.item(), 'loss_image': loss_image.item()}

# æµ‹è¯•
if __name__ == '__main__':
    aux_loss = AuxiliaryAspectLoss()
    
    # æ¨¡æ‹Ÿé¢„æµ‹
    aspect_logits_text = torch.randn(4, 3)
    aspect_logits_image = torch.randn(4, 3)
    aspect_ids = torch.tensor([0, 1, 2, 0])
    
    loss, info = aux_loss(aspect_logits_text, aspect_logits_image, aspect_ids)
    print(f"è¾…åŠ©æŸå¤±: {loss.item():.4f}")
    print(f"æ–‡æœ¬æŸå¤±: {info['loss_text']:.4f}")
    print(f"å›¾åƒæŸå¤±: {info['loss_image']:.4f}")
```

### 4.3 æ”¹è¿›çš„Aspect-Aware SupConæŸå¤±

**å…³é”®æ”¹è¿›ï¼š** åªæœ‰æƒ…æ„Ÿ**ä¸”**æ–¹é¢éƒ½ç›¸åŒæ‰æ˜¯æ­£ä¾‹ï¼Œé¿å…"åŒæ–¹é¢ä¸åŒæƒ…æ„Ÿ"è¢«æ‹‰è¿‘ã€‚

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/losses/supcon.py`**
- **ç”¨é€”**: å®ç°æ–¹é¢æ„ŸçŸ¥çš„ç›‘ç£å¯¹æ¯”æŸå¤± `AspectAwareSupConLoss` ä»¥åŠå¤šè§†å›¾ç‰ˆæœ¬ `MultiViewSupConLoss`ï¼Œåœ¨å¯¹æ¯”å­¦ä¹ ä¸­åŒæ—¶è€ƒè™‘æƒ…æ„Ÿå’Œæ–¹é¢ä¸€è‡´æ€§ã€‚
- **è¾“å…¥**: å½’ä¸€åŒ–åçš„ç‰¹å¾ `features` æˆ– `(h_text, h_image)`ï¼Œå¯¹åº”çš„æƒ…æ„Ÿæ ‡ç­¾ `labels` å’Œæ–¹é¢ ID `aspect_ids`ã€‚
- **è¾“å‡º**: æ ‡é‡æŸå¤±å€¼ï¼Œç”¨äºæ‹‰è¿‘â€œåŒæƒ…æ„Ÿä¸”åŒæ–¹é¢â€çš„æ ·æœ¬ã€æ¨è¿œå…¶ä»–æ ·æœ¬ï¼Œå¹¶åœ¨å¤šè§†å›¾åœºæ™¯ä¸‹åŒæ—¶çº¦æŸæ–‡æœ¬å’Œå›¾åƒç‰¹å¾ã€‚

```python
# src/losses/supcon.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class AspectAwareSupConLoss(nn.Module):
    """æ–¹é¢æ„ŸçŸ¥çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æŸå¤±"""
    
    def __init__(self, temperature=0.1, base_temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.base_temperature = base_temperature
    
    def forward(self, features, labels, aspect_ids):
        """
        Args:
            features: [B, D'] å½’ä¸€åŒ–çš„ç‰¹å¾ï¼ˆh_textæˆ–h_imageï¼‰
            labels: [B] æƒ…æ„Ÿæ ‡ç­¾
            aspect_ids: [B] æ–¹é¢ID
        
        Returns:
            loss: scalar
        """
        device = features.device
        batch_size = features.shape[0]
        
        # 1. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
        sim_matrix = torch.matmul(features, features.T) / self.temperature
        
        # 2. å®šä¹‰æ­£æ ·æœ¬maskï¼šæƒ…æ„Ÿç›¸åŒ AND æ–¹é¢ç›¸åŒ (æ’é™¤è‡ªå·±)
        label_match = labels.unsqueeze(1) == labels.unsqueeze(0)  # [B, B]
        aspect_match = aspect_ids.unsqueeze(1) == aspect_ids.unsqueeze(0)  # [B, B]
        pos_mask = (label_match & aspect_match).float()
        pos_mask.fill_diagonal_(0)  # æ’é™¤è‡ªå·±
        
        # 3. å®šä¹‰ç¡¬è´Ÿä¾‹æƒé‡
        weights = torch.ones_like(sim_matrix)
        
        # æƒ…å†µA: åŒæ–¹é¢ã€å¼‚æƒ…æ„Ÿï¼ˆæœ€éš¾è´Ÿä¾‹ï¼‰-> æƒé‡ 2.0
        hard_senti_mask = aspect_match & (~label_match)
        weights[hard_senti_mask] = 2.0
        
        # æƒ…å†µB: åŒæƒ…æ„Ÿã€å¼‚æ–¹é¢ï¼ˆæ–¹é¢æ··æ·†ï¼‰-> æƒé‡ 1.5
        hard_aspect_mask = label_match & (~aspect_match)
        weights[hard_aspect_mask] = 1.5
        
        # 4. è®¡ç®—å¯¹æ¯”æŸå¤±
        # ä¸ºæ•°å€¼ç¨³å®šæ€§ï¼Œå‡å»æœ€å¤§å€¼
        logits_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)
        logits = sim_matrix - logits_max.detach()
        
        # è®¡ç®—åŠ æƒçš„exp
        exp_logits = torch.exp(logits) * weights
        
        # åˆ†æ¯ï¼šæ‰€æœ‰è´Ÿæ ·æœ¬çš„åŠ æƒå’Œï¼ˆæ’é™¤è‡ªå·±ï¼‰
        mask_self = torch.eye(batch_size, device=device).bool()
        exp_logits.masked_fill_(mask_self, 0)
        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-8)
        
        # å¯¹æ¯ä¸ªæ­£æ ·æœ¬è®¡ç®—æŸå¤±
        # åªå¯¹æœ‰æ­£æ ·æœ¬çš„æ ·æœ¬è®¡ç®—
        pos_per_sample = pos_mask.sum(dim=1)
        valid_samples = pos_per_sample > 0
        
        if valid_samples.sum() == 0:
            # å¦‚æœbatchä¸­æ²¡æœ‰æ­£æ ·æœ¬å¯¹ï¼Œè¿”å›0
            return torch.tensor(0.0, device=device)
        
        # è®¡ç®—å¹³å‡å¯¹æ•°æ¦‚ç‡
        mean_log_prob_pos = (pos_mask * log_prob).sum(dim=1) / (pos_per_sample + 1e-8)
        
        # åªå¯¹æœ‰æ­£æ ·æœ¬çš„æ ·æœ¬è®¡ç®—æŸå¤±
        loss = -mean_log_prob_pos[valid_samples].mean()
        
        return loss * (self.temperature / self.base_temperature)

class MultiViewSupConLoss(nn.Module):
    """å¤šè§†å›¾SupConï¼šåŒæ—¶å¯¹æ–‡æœ¬å’Œå›¾åƒç‰¹å¾åšå¯¹æ¯”"""
    
    def __init__(self, temperature=0.1):
        super().__init__()
        self.supcon = AspectAwareSupConLoss(temperature=temperature)
    
    def forward(self, h_text, h_image, labels, aspect_ids):
        """
        Args:
            h_text: [B, D'] æ–‡æœ¬æŠ•å½±ç‰¹å¾
            h_image: [B, D'] å›¾åƒæŠ•å½±ç‰¹å¾
            labels: [B] æƒ…æ„Ÿæ ‡ç­¾
            aspect_ids: [B] æ–¹é¢ID
        
        Returns:
            loss: scalar
        """
        # å †å ä¸ºå¤šè§†å›¾ [2B, D']
        features = torch.cat([h_text, h_image], dim=0)
        labels = torch.cat([labels, labels], dim=0)
        aspect_ids = torch.cat([aspect_ids, aspect_ids], dim=0)
        
        loss = self.supcon(features, labels, aspect_ids)
        
        return loss

# æµ‹è¯•
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    supcon_loss = MultiViewSupConLoss(temperature=0.1).to(device)
    
    # æ¨¡æ‹Ÿç‰¹å¾
    h_text = F.normalize(torch.randn(8, 256), p=2, dim=1).to(device)
    h_image = F.normalize(torch.randn(8, 256), p=2, dim=1).to(device)
    labels = torch.tensor([0, 0, 1, 1, 2, 2, 0, 1]).to(device)
    aspect_ids = torch.tensor([0, 0, 1, 1, 2, 2, 0, 1]).to(device)
    
    loss = supcon_loss(h_text, h_image, labels, aspect_ids)
    print(f"Aspect-Aware SupConæŸå¤±: {loss.item():.4f}")
    print(f"âœ… SupConæŸå¤±è®¡ç®—æˆåŠŸ")
```

### 4.4 InfoNCEæŸå¤±ï¼ˆå¸¦Pair-ID Maskï¼‰

**å…³é”®è®¾è®¡ï¼š** å¿…é¡»å±è”½åŒä¸€å›¾æ–‡å¯¹çš„ä¸åŒæ–¹é¢æ ·æœ¬ï¼Œé¿å…å®ƒä»¬äº’ä¸ºè´Ÿæ ·æœ¬ã€‚

**å®ç°ä»£ç ï¼š**

**æ¨¡å—è¯´æ˜ï¼š`src/losses/infonce.py`**
- **ç”¨é€”**: å®ç°å¸¦ `pair_id` æ©ç çš„è·¨æ¨¡æ€ InfoNCE æŸå¤± `InfoNCELoss`ï¼Œç”¨äºå¯¹é½åŒä¸€å›¾æ–‡å¯¹çš„æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºï¼ŒåŒæ—¶é¿å…åŒä¸€å›¾æ–‡å¯¹çš„ä¸åŒæ–¹é¢è¯¯å½“ä½œè´Ÿæ ·æœ¬ã€‚
- **è¾“å…¥**: å½’ä¸€åŒ–çš„æ–‡æœ¬ç‰¹å¾ `h_text`ã€å›¾åƒç‰¹å¾ `h_image`ï¼ˆå½¢çŠ¶ `[B, D']`ï¼‰ï¼Œä»¥åŠ `pair_id_mask`ï¼ˆå½¢çŠ¶ `[B, B]`ï¼Œ`True` è¡¨ç¤ºåŒä¸€å›¾æ–‡å¯¹ï¼‰ã€‚
- **è¾“å‡º**: æ ‡é‡æŸå¤±å€¼ `loss`ï¼Œä»¥åŠ text-to-image / image-to-text ä¸¤ä¸ªæ–¹å‘çš„æŸå¤±æ˜ç»†å­—å…¸ã€‚

```python
# src/losses/infonce.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class InfoNCELoss(nn.Module):
    """InfoNCEè·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼ˆå¸¦Pair-IDæ©ç ï¼‰"""
    
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, h_text, h_image, pair_id_mask):
        """
        Args:
            h_text: [B, D'] L2å½’ä¸€åŒ–çš„æ–‡æœ¬ç‰¹å¾
            h_image: [B, D'] L2å½’ä¸€åŒ–çš„å›¾åƒç‰¹å¾
            pair_id_mask: [B, B] boolçŸ©é˜µï¼ŒTrueè¡¨ç¤ºç›¸åŒpair_idï¼ˆéœ€æ’é™¤ï¼‰
        
        Returns:
            loss: scalar
        """
        device = h_text.device
        batch_size = h_text.shape[0]
        
        # 1. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # text-to-image: [B, B]
        sim_t2i = torch.matmul(h_text, h_image.T) / self.temperature
        # image-to-text: [B, B]
        sim_i2t = torch.matmul(h_image, h_text.T) / self.temperature
        
        # 2. æ„å»ºæ­£æ ·æœ¬maskï¼ˆå¯¹è§’çº¿ï¼‰
        pos_mask = torch.eye(batch_size, device=device).bool()
        
        # 3. æ„å»ºè´Ÿæ ·æœ¬maskï¼ˆæ’é™¤è‡ªå·±å’Œç›¸åŒpair_idï¼‰
        # è´Ÿæ ·æœ¬ = ä¸æ˜¯è‡ªå·± AND ä¸æ˜¯åŒä¸€pair_id
        neg_mask_t2i = ~(pos_mask | pair_id_mask)
        neg_mask_i2t = ~(pos_mask | pair_id_mask)
        
        # 4. è®¡ç®—text-to-imageæŸå¤±
        # åˆ†å­ï¼šæ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim_t2i = sim_t2i.diagonal()  # [B]
        
        # åˆ†æ¯ï¼šæ­£æ ·æœ¬ + æ‰€æœ‰æœ‰æ•ˆè´Ÿæ ·æœ¬
        # ä¸ºæ•°å€¼ç¨³å®šï¼Œå‡å»æœ€å¤§å€¼
        logits_max_t2i, _ = torch.max(sim_t2i, dim=1, keepdim=True)
        exp_sim_t2i = torch.exp(sim_t2i - logits_max_t2i.detach())
        
        # åªä¿ç•™æœ‰æ•ˆçš„è´Ÿæ ·æœ¬
        exp_sim_t2i = exp_sim_t2i * neg_mask_t2i.float()
        # åŠ ä¸Šæ­£æ ·æœ¬
        exp_sim_t2i.diagonal().copy_(torch.exp(pos_sim_t2i - logits_max_t2i.squeeze()))
        
        denominator_t2i = exp_sim_t2i.sum(dim=1)
        loss_t2i = -torch.log(
            torch.exp(pos_sim_t2i - logits_max_t2i.squeeze()) / (denominator_t2i + 1e-8)
        ).mean()
        
        # 5. è®¡ç®—image-to-textæŸå¤±ï¼ˆå¯¹ç§°ï¼‰
        pos_sim_i2t = sim_i2t.diagonal()
        logits_max_i2t, _ = torch.max(sim_i2t, dim=1, keepdim=True)
        exp_sim_i2t = torch.exp(sim_i2t - logits_max_i2t.detach())
        exp_sim_i2t = exp_sim_i2t * neg_mask_i2t.float()
        exp_sim_i2t.diagonal().copy_(torch.exp(pos_sim_i2t - logits_max_i2t.squeeze()))
        
        denominator_i2t = exp_sim_i2t.sum(dim=1)
        loss_i2t = -torch.log(
            torch.exp(pos_sim_i2t - logits_max_i2t.squeeze()) / (denominator_i2t + 1e-8)
        ).mean()
        
        # 6. åŒå‘å¹³å‡
        loss = (loss_t2i + loss_i2t) / 2
        
        return loss, {'loss_t2i': loss_t2i.item(), 'loss_i2t': loss_i2t.item()}

# æµ‹è¯•
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    infonce_loss = InfoNCELoss(temperature=0.07).to(device)
    
    # æ¨¡æ‹Ÿç‰¹å¾ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
    h_text = F.normalize(torch.randn(8, 256), p=2, dim=1).to(device)
    h_image = F.normalize(torch.randn(8, 256), p=2, dim=1).to(device)
    
    # æ¨¡æ‹Ÿpair_id_maskï¼šæ ·æœ¬0å’Œ1å…±äº«ç›¸åŒpair_id
    pair_id_mask = torch.zeros(8, 8, dtype=torch.bool).to(device)
    pair_id_mask[0, 1] = True
    pair_id_mask[1, 0] = True
    
    loss, info = infonce_loss(h_text, h_image, pair_id_mask)
    print(f"InfoNCEæŸå¤±: {loss.item():.4f}")
    print(f"Text-to-ImageæŸå¤±: {info['loss_t2i']:.4f}")
    print(f"Image-to-TextæŸå¤±: {info['loss_i2t']:.4f}")
    print(f"âœ… InfoNCEæŸå¤±è®¡ç®—æˆåŠŸ")
```

### 4.5 åˆ†ç±»æŸå¤±

**æ¨¡å—è¯´æ˜ï¼š`src/losses/classification.py`**
- **ç”¨é€”**: å°è£…åŸºç¡€çš„æƒ…æ„Ÿåˆ†ç±»äº¤å‰ç†µæŸå¤± `ClassificationLoss`ï¼Œå¯é€‰æ”¯æŒ label smoothingã€‚
- **è¾“å…¥**: æƒ…æ„Ÿé¢„æµ‹ logitsï¼ˆå½¢çŠ¶ `[B, 3]`ï¼‰å’ŒçœŸå®æƒ…æ„Ÿæ ‡ç­¾ `labels`ï¼ˆ`[B]`ï¼‰ã€‚
- **è¾“å‡º**: æ ‡é‡åˆ†ç±»æŸå¤±ï¼Œç”¨äºç›´æ¥ä¼˜åŒ–æƒ…æ„Ÿé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

```python
# src/losses/classification.py
import torch
import torch.nn as nn

class ClassificationLoss(nn.Module):
    """æƒ…æ„Ÿåˆ†ç±»æŸå¤±"""
    
    def __init__(self, label_smoothing=0.0):
        super().__init__()
        self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    
    def forward(self, logits, labels):
        """
        Args:
            logits: [B, 3] æƒ…æ„Ÿé¢„æµ‹logits
            labels: [B] çœŸå®æ ‡ç­¾
        
        Returns:
            loss: scalar
        """
        return self.criterion(logits, labels)
```

### 4.6 è”åˆæŸå¤±å‡½æ•°

**æ¨¡å—è¯´æ˜ï¼š`src/losses/total_loss.py`**
- **ç”¨é€”**: å°†åˆ†ç±»æŸå¤±ã€InfoNCE æŸå¤±ã€SupCon æŸå¤±å’Œè¾…åŠ©ä»»åŠ¡æŸå¤±ç»„åˆä¸ºä¸€ä¸ªæ€»æŸå¤± `TotalLoss`ï¼Œå¹¶æ”¯æŒé€šè¿‡è¶…å‚æ•°æ§åˆ¶å„é¡¹æƒé‡ã€‚
- **è¾“å…¥**: æ¨¡å‹è¾“å‡ºå­—å…¸ `outputs`ï¼ˆåŒ…å« logits å’Œå¯¹æ¯”å­¦ä¹ ç‰¹å¾ï¼‰ã€æƒ…æ„Ÿæ ‡ç­¾ `labels`ã€æ–¹é¢ ID `aspect_ids`ã€`pair_id_mask`ã€‚
- **è¾“å‡º**: æ ‡é‡æ€»æŸå¤± `total_loss`ï¼Œä»¥åŠåŒ…å«å„å­æŸå¤±ä¸åˆ†é¡¹ä¿¡æ¯çš„å­—å…¸ `loss_dict`ï¼Œä¾¿äºç›‘æ§å’Œè°ƒå‚ã€‚

```python
# src/losses/total_loss.py
import torch
import torch.nn as nn
from classification import ClassificationLoss
from infonce import InfoNCELoss
from supcon import MultiViewSupConLoss
from auxiliary import AuxiliaryAspectLoss

class TotalLoss(nn.Module):
    """è”åˆæŸå¤±å‡½æ•°"""
    
    def __init__(
        self,
        alpha=1.0,  # InfoNCEæƒé‡
        beta=0.5,   # SupConæƒé‡
        gamma=0.3,  # è¾…åŠ©ä»»åŠ¡æƒé‡
        temperature_infonce=0.07,
        temperature_supcon=0.1
    ):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        
        # å„ä¸ªæŸå¤±å‡½æ•°
        self.cls_loss = ClassificationLoss()
        self.infonce_loss = InfoNCELoss(temperature=temperature_infonce)
        self.supcon_loss = MultiViewSupConLoss(temperature=temperature_supcon)
        self.aux_loss = AuxiliaryAspectLoss()
    
    def forward(self, outputs, labels, aspect_ids, pair_id_mask):
        """
        Args:
            outputs: æ¨¡å‹è¾“å‡ºå­—å…¸
            labels: [B] æƒ…æ„Ÿæ ‡ç­¾
            aspect_ids: [B] æ–¹é¢ID
            pair_id_mask: [B, B] pair_idæ©ç 
        
        Returns:
            total_loss: scalar
            loss_dict: å„é¡¹æŸå¤±çš„å­—å…¸
        """
        # 1. åˆ†ç±»æŸå¤±
        loss_cls = self.cls_loss(outputs['sentiment_logits'], labels)
        
        # 2. InfoNCEæŸå¤±
        loss_infonce, infonce_info = self.infonce_loss(
            outputs['h_text'],
            outputs['h_image'],
            pair_id_mask
        )
        
        # 3. SupConæŸå¤±
        loss_supcon = self.supcon_loss(
            outputs['h_text'],
            outputs['h_image'],
            labels,
            aspect_ids
        )
        
        # 4. è¾…åŠ©ä»»åŠ¡æŸå¤±
        loss_aux, aux_info = self.aux_loss(
            outputs['aspect_logits_text'],
            outputs['aspect_logits_image'],
            aspect_ids
        )
        
        # 5. æ€»æŸå¤±
        total_loss = (
            loss_cls +
            self.alpha * loss_infonce +
            self.beta * loss_supcon +
            self.gamma * loss_aux
        )
        
        # æŸå¤±å­—å…¸
        loss_dict = {
            'total': total_loss.item(),
            'cls': loss_cls.item(),
            'infonce': loss_infonce.item(),
            'infonce_t2i': infonce_info['loss_t2i'],
            'infonce_i2t': infonce_info['loss_i2t'],
            'supcon': loss_supcon.item(),
            'aux': loss_aux.item(),
            'aux_text': aux_info['loss_text'],
            'aux_image': aux_info['loss_image']
        }
        
        return total_loss, loss_dict

# æµ‹è¯•
if __name__ == '__main__':
    import torch.nn.functional as F
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    total_loss_fn = TotalLoss(
        alpha=1.0,
        beta=0.5,
        gamma=0.3
    ).to(device)
    
    # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º
    batch_size = 8
    outputs = {
        'sentiment_logits': torch.randn(batch_size, 3).to(device),
        'aspect_logits_text': torch.randn(batch_size, 3).to(device),
        'aspect_logits_image': torch.randn(batch_size, 3).to(device),
        'h_text': F.normalize(torch.randn(batch_size, 256), p=2, dim=1).to(device),
        'h_image': F.normalize(torch.randn(batch_size, 256), p=2, dim=1).to(device)
    }
    
    labels = torch.randint(0, 3, (batch_size,)).to(device)
    aspect_ids = torch.randint(0, 3, (batch_size,)).to(device)
    pair_id_mask = torch.zeros(batch_size, batch_size, dtype=torch.bool).to(device)
    
    # è®¡ç®—æŸå¤±
    total_loss, loss_dict = total_loss_fn(outputs, labels, aspect_ids, pair_id_mask)
    
    print("=== æŸå¤±å‡½æ•°æµ‹è¯• ===")
    print(f"æ€»æŸå¤±: {loss_dict['total']:.4f}")
    print(f"  åˆ†ç±»æŸå¤±: {loss_dict['cls']:.4f}")
    print(f"  InfoNCEæŸå¤±: {loss_dict['infonce']:.4f}")
    print(f"    - T2I: {loss_dict['infonce_t2i']:.4f}")
    print(f"    - I2T: {loss_dict['infonce_i2t']:.4f}")
    print(f"  SupConæŸå¤±: {loss_dict['supcon']:.4f}")
    print(f"  è¾…åŠ©æŸå¤±: {loss_dict['aux']:.4f}")
    print(f"    - Text: {loss_dict['aux_text']:.4f}")
    print(f"    - Image: {loss_dict['aux_image']:.4f}")
    print(f"âœ… è”åˆæŸå¤±å‡½æ•°æµ‹è¯•æˆåŠŸ")
```

**è¿è¡Œå®Œæ•´æµ‹è¯•ï¼š**
```bash
cd src/losses
python total_loss.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
=== æŸå¤±å‡½æ•°æµ‹è¯• ===
æ€»æŸå¤±: 3.2456
  åˆ†ç±»æŸå¤±: 1.0987
  InfoNCEæŸå¤±: 1.2345
    - T2I: 1.2123
    - I2T: 1.2567
  SupConæŸå¤±: 0.7654
  è¾…åŠ©æŸå¤±: 0.4321
    - Text: 0.4234
    - Image: 0.4408
âœ… è”åˆæŸå¤±å‡½æ•°æµ‹è¯•æˆåŠŸ
```

### 4.7 æŸå¤±æƒé‡è°ƒä¼˜å»ºè®®

**é»˜è®¤æƒé‡é…ç½®ï¼š**

| æŸå¤±é¡¹ | æƒé‡ | ä½œç”¨ | è°ƒä¼˜å»ºè®® |
|--------|------|------|---------|
| L_cls | 1.0 (åŸºå‡†) | æƒ…æ„Ÿåˆ†ç±» | ä¿æŒä¸º1.0 |
| L_InfoNCE (Î±) | 1.0 | è·¨æ¨¡æ€å¯¹é½ | å¦‚æœå¯¹é½ä¸è¶³ï¼Œå¢å¤§åˆ°1.5-2.0 |
| L_SupCon (Î²) | 0.5 | æƒ…æ„Ÿå¯åˆ†ç¦» | å¦‚æœæƒ…æ„Ÿæ··æ·†ä¸¥é‡ï¼Œå¢å¤§åˆ°0.8-1.0 |
| L_aux (Î³) | 0.3 | æ–¹é¢è¯†åˆ« | å¦‚æœæ–¹é¢æ··æ·†ï¼Œå¢å¤§åˆ°0.5 |

**åŠ¨æ€è°ƒæ•´ç­–ç•¥ï¼š**

```python
# è®­ç»ƒåˆæœŸï¼ˆEpoch 1-3ï¼‰ï¼šå¼ºåŒ–åŸºç¡€å¯¹é½
alpha, beta, gamma = 1.5, 0.3, 0.2

# è®­ç»ƒä¸­æœŸï¼ˆEpoch 4-10ï¼‰ï¼šå¹³è¡¡å‘å±•
alpha, beta, gamma = 1.0, 0.5, 0.3

# è®­ç»ƒåæœŸï¼ˆEpoch 11-15ï¼‰ï¼šå¼ºåŒ–åˆ†ç±»
alpha, beta, gamma = 0.8, 0.5, 0.2
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] æ‰€æœ‰æŸå¤±å‡½æ•°å•ç‹¬æµ‹è¯•é€šè¿‡
- [ ] è”åˆæŸå¤±è®¡ç®—æ­£ç¡®
- [ ] Pair-ID maskæ­£ç¡®åº”ç”¨
- [ ] Aspect-Awareæ­£æ ·æœ¬å®šä¹‰æ­£ç¡®
- [ ] ç¡¬è´Ÿä¾‹æƒé‡æœºåˆ¶å·¥ä½œ

**é¢„è®¡è€—æ—¶ï¼š** 2-3å°æ—¶

---

## 5. è®­ç»ƒæ‰§è¡Œæµç¨‹

### 5.1 è®­ç»ƒå™¨å®ç°

**æ¨¡å—è¯´æ˜ï¼š`src/training/trainer.py`**
- **ç”¨é€”**: å°è£…å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ / å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»ºã€å•ä¸ª epoch è®­ç»ƒã€éªŒè¯è¯„ä¼°ã€æ··åˆç²¾åº¦ã€æ¢¯åº¦è£å‰ªä»¥åŠæ£€æŸ¥ç‚¹ä¿å­˜ç­‰ã€‚
- **è¾“å…¥**: å·²æ„å»ºå¥½çš„æ¨¡å‹ `model`ã€`train_loader`ã€`dev_loader`ã€æŸå¤±å‡½æ•° `loss_fn`ã€è¿è¡Œè®¾å¤‡ `device`ã€è®­ç»ƒé…ç½® `config`ï¼ˆåŒ…æ‹¬å­¦ä¹ ç‡ã€epoch æ•°ã€ä¿å­˜ç›®å½•ç­‰ï¼‰ã€‚
- **è¾“å‡º**: å†…éƒ¨ç»´æŠ¤è®­ç»ƒ / éªŒè¯å†å²ï¼ˆaccuracyã€F1ã€å„é¡¹æŸå¤±ï¼‰ï¼Œå¯¹å¤–é€šè¿‡ `train()` æ–¹æ³•æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨ç£ç›˜ä¸Šä¿å­˜æœ€ä½³æ¨¡å‹å’Œä¸­é—´æ£€æŸ¥ç‚¹ã€‚

```python
# src/training/trainer.py
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import get_linear_schedule_with_warmup
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import os

class Trainer:
    """AGPæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model,
        train_loader,
        dev_loader,
        loss_fn,
        device,
        config
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.dev_loader = dev_loader
        self.loss_fn = loss_fn.to(device)
        self.device = device
        self.config = config
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer, self.scheduler = self._create_optimizer()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config['use_amp'] else None
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.global_step = 0
        self.best_dev_f1 = 0.0
        
        # æ—¥å¿—
        self.train_history = []
        self.dev_history = []
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåˆ†å±‚å­¦ä¹ ç‡ï¼‰"""
        # åˆ†ç»„å‚æ•°
        backbone_params = []
        new_params = []
        
        # BERTå‚æ•°ï¼ˆå†»ç»“çš„å±‚ä¸åŠ å…¥ä¼˜åŒ–ï¼‰
        for name, param in self.model.text_encoder.named_parameters():
            if param.requires_grad:
                backbone_params.append(param)
        
        # ViT LoRAå‚æ•°
        for name, param in self.model.image_encoder.named_parameters():
            if param.requires_grad:
                backbone_params.append(param)
        
        # æ–°å¢æ¨¡å—å‚æ•°
        for module in [
            self.model.query_generator,
            self.model.text_cross_attn,
            self.model.image_cross_attn,
            self.model.text_pooling,
            self.model.image_pooling,
            self.model.text_proj,
            self.model.image_proj,
            self.model.sentiment_classifier,
            self.model.aspect_classifier_text,
            self.model.aspect_classifier_image
        ]:
            new_params.extend(list(module.parameters()))
        
        # ä¼˜åŒ–å™¨é…ç½®
        optimizer = AdamW([
            {'params': backbone_params, 'lr': self.config['lr_backbone']},
            {'params': new_params, 'lr': self.config['lr_head']}
        ], weight_decay=self.config['weight_decay'])
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¸¦warmupï¼‰
        num_training_steps = len(self.train_loader) * self.config['num_epochs']
        num_warmup_steps = int(num_training_steps * self.config['warmup_ratio'])
        
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
        
        print(f"âœ… ä¼˜åŒ–å™¨é…ç½®:")
        print(f"  Backbone LR: {self.config['lr_backbone']}")
        print(f"  Head LR: {self.config['lr_head']}")
        print(f"  Warmup steps: {num_warmup_steps}")
        print(f"  Total steps: {num_training_steps}")
        
        return optimizer, scheduler
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = []
        epoch_metrics = []
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.epoch+1} [Train]")
        
        for batch_idx, batch in enumerate(pbar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
            if self.scaler:
                with autocast():
                    outputs = self.model(batch)
                    loss, loss_dict = self.loss_fn(
                        outputs,
                        batch['labels'],
                        batch['aspect_ids'],
                        batch['pair_id_mask']
                    )
            else:
                outputs = self.model(batch)
                loss, loss_dict = self.loss_fn(
                    outputs,
                    batch['labels'],
                    batch['aspect_ids'],
                    batch['pair_id_mask']
                )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            if self.scaler:
                self.scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ª
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['max_grad_norm']
                )
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['max_grad_norm']
                )
                self.optimizer.step()
            
            self.scheduler.step()
            self.global_step += 1
            
            # è®¡ç®—å‡†ç¡®ç‡
            preds = outputs['sentiment_logits'].argmax(dim=1)
            acc = (preds == batch['labels']).float().mean().item()
            
            epoch_losses.append(loss_dict)
            epoch_metrics.append({'acc': acc})
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': loss_dict['total'],
                'acc': acc,
                'lr': self.scheduler.get_last_lr()[0]
            })
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_loss = {k: np.mean([d[k] for d in epoch_losses]) 
                   for k in epoch_losses[0].keys()}
        avg_acc = np.mean([m['acc'] for m in epoch_metrics])
        
        return avg_loss, avg_acc
    
    @torch.no_grad()
    def evaluate(self):
        """éªŒè¯é›†è¯„ä¼°"""
        self.model.eval()
        all_preds = []
        all_labels = []
        epoch_losses = []
        
        pbar = tqdm(self.dev_loader, desc=f"Epoch {self.epoch+1} [Dev]")
        
        for batch in pbar:
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            outputs = self.model(batch)
            loss, loss_dict = self.loss_fn(
                outputs,
                batch['labels'],
                batch['aspect_ids'],
                batch['pair_id_mask']
            )
            
            preds = outputs['sentiment_logits'].argmax(dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
            epoch_losses.append(loss_dict)
        
        # è®¡ç®—æŒ‡æ ‡
        acc = accuracy_score(all_labels, all_preds)
        macro_f1 = f1_score(all_labels, all_preds, average='macro')
        weighted_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        avg_loss = {k: np.mean([d[k] for d in epoch_losses]) 
                   for k in epoch_losses[0].keys()}
        
        return avg_loss, {'acc': acc, 'macro_f1': macro_f1, 'weighted_f1': weighted_f1}
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è®­ç»ƒ - {self.config['num_epochs']} epochs")
        print(f"{'='*60}\n")
        
        for epoch in range(self.config['num_epochs']):
            self.epoch = epoch
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()
            
            # éªŒè¯
            dev_loss, dev_metrics = self.evaluate()
            
            # è®°å½•å†å²
            self.train_history.append({
                'epoch': epoch + 1,
                'loss': train_loss,
                'acc': train_acc
            })
            self.dev_history.append({
                'epoch': epoch + 1,
                'loss': dev_loss,
                **dev_metrics
            })
            
            # æ‰“å°æ‘˜è¦
            print(f"\n--- Epoch {epoch+1} Summary ---")
            print(f"Train Loss: {train_loss['total']:.4f} | Train Acc: {train_acc:.4f}")
            print(f"Dev Loss: {dev_loss['total']:.4f} | Dev Acc: {dev_metrics['acc']:.4f} | "
                  f"Dev Macro-F1: {dev_metrics['macro_f1']:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if dev_metrics['macro_f1'] > self.best_dev_f1:
                self.best_dev_f1 = dev_metrics['macro_f1']
                self.save_checkpoint(
                    os.path.join(self.config['save_dir'], 'best_model.pt'),
                    is_best=True
                )
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {self.best_dev_f1:.4f})")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config['save_every'] == 0:
                self.save_checkpoint(
                    os.path.join(self.config['save_dir'], f'checkpoint_epoch_{epoch+1}.pt')
                )
        
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³Dev F1: {self.best_dev_f1:.4f}")
        print(f"{'='*60}\n")
    
    def save_checkpoint(self, path, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_dev_f1': self.best_dev_f1,
            'config': self.config,
            'train_history': self.train_history,
            'dev_history': self.dev_history
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint, path)
        print(f"{'Best model' if is_best else 'Checkpoint'} saved to {path}")
```

### 5.2 ä¸»è®­ç»ƒè„šæœ¬

**æ¨¡å—è¯´æ˜ï¼š`train.py`**
- **ç”¨é€”**: ä½œä¸ºè®­ç»ƒå…¥å£è„šæœ¬ï¼Œè´Ÿè´£åŠ è½½é…ç½®ã€æ„å»ºæ•°æ®åŠ è½½å™¨å’Œæ¨¡å‹ã€åˆ›å»ºæŸå¤±å‡½æ•°å’Œè®­ç»ƒå™¨ï¼Œå¹¶ä¸²è”èµ·æ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚
- **è¾“å…¥**: `configs/training_config.yaml` é…ç½®æ–‡ä»¶ï¼ˆæˆ–å‘½ä»¤è¡Œ / å¤–éƒ¨ç¯å¢ƒæä¾›çš„è·¯å¾„ï¼‰ï¼Œä»¥åŠé¡¹ç›®ç›®å½•ä¸‹çš„æ•°æ®å’Œæ¨¡å‹æ–‡ä»¶ã€‚
- **è¾“å‡º**: è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¿å­˜åˆ° `models/checkpoints`ï¼‰ã€è®­ç»ƒæ—¥å¿—ï¼ˆæ§åˆ¶å°å’Œå¯é€‰çš„ JSON å†å²æ–‡ä»¶ï¼‰ï¼Œä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰“å°çš„å…³é”®ä¿¡æ¯ã€‚

```python
# train.py
import torch
import yaml
import os
import random
import numpy as np
from src.data.create_dataloaders import create_dataloaders
from src.models.agp_model import AGPModel
from src.losses.total_loss import TotalLoss
from src.training.trainer import Trainer

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def load_config(config_path='configs/training_config.yaml'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def main():
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config['seed'])
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config['save_dir'], exist_ok=True)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n1. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, dev_loader, test_loader, num_aspects = create_dataloaders(
        train_jsonl=config['train_jsonl'],
        dev_jsonl=config['dev_jsonl'],
        test_jsonl=config['test_jsonl'],
        image_root=config['image_root'],
        batch_size=config['batch_size'],
        num_workers=config['num_workers']
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºæ¨¡å‹...")
    model = AGPModel(
        num_aspects=num_aspects,
        hidden_dim=config['hidden_dim'],
        proj_dim=config['proj_dim'],
        num_queries=config['num_queries'],
        num_classes=config['num_classes'],
        freeze_bert_layers=config['freeze_bert_layers'],
        use_lora=config['use_lora'],
        lora_rank=config['lora_rank']
    )
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    print("\n3. åˆ›å»ºæŸå¤±å‡½æ•°...")
    loss_fn = TotalLoss(
        alpha=config['alpha'],
        beta=config['beta'],
        gamma=config['gamma'],
        temperature_infonce=config['temperature_infonce'],
        temperature_supcon=config['temperature_supcon']
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n4. åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        dev_loader=dev_loader,
        loss_fn=loss_fn,
        device=device,
        config=config
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\n5. å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜è®­ç»ƒå†å²
    import json
    history_path = os.path.join(config['save_dir'], 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump({
            'train': trainer.train_history,
            'dev': trainer.dev_history
        }, f, indent=2)
    print(f"\nè®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

if __name__ == '__main__':
    main()
```

### 5.3 é…ç½®æ–‡ä»¶

```yaml
# configs/training_config.yaml
# æ•°æ®è·¯å¾„
train_jsonl: 'data/processed/train_expanded.jsonl'
dev_jsonl: 'data/processed/dev_expanded.jsonl'
test_jsonl: 'data/processed/test_expanded.jsonl'
image_root: 'data/images'

# æ¨¡å‹é…ç½®
hidden_dim: 768
proj_dim: 256
num_queries: 8
num_classes: 3
freeze_bert_layers: 10
use_lora: true
lora_rank: 8

# æŸå¤±æƒé‡
alpha: 1.0          # InfoNCE
beta: 0.5           # SupCon
gamma: 0.3          # Auxiliary
temperature_infonce: 0.07
temperature_supcon: 0.1

# è®­ç»ƒé…ç½®
num_epochs: 15
batch_size: 32
num_workers: 4
lr_backbone: 1.0e-5  # BERTå’ŒLoRA
lr_head: 1.0e-4      # æ–°æ¨¡å—
weight_decay: 0.01
warmup_ratio: 0.1
max_grad_norm: 1.0
use_amp: true        # æ··åˆç²¾åº¦è®­ç»ƒ

# ä¿å­˜é…ç½®
save_dir: 'models/checkpoints'
save_every: 5

# å…¶ä»–
seed: 42
```

### 5.4 æ‰§è¡Œè®­ç»ƒ

**æ­¥éª¤1ï¼šéªŒè¯é…ç½®**

```bash
# æ£€æŸ¥é…ç½®æ–‡ä»¶
cat configs/training_config.yaml

# æµ‹è¯•æ•°æ®åŠ è½½
python -c "from src.data.create_dataloaders import create_dataloaders; \
           train_loader, dev_loader, test_loader, num_aspects = create_dataloaders(batch_size=8); \
           print(f'Train: {len(train_loader)}, Dev: {len(dev_loader)}, Test: {len(test_loader)}')"
```

**æ­¥éª¤2ï¼šå¯åŠ¨è®­ç»ƒ**

```bash
# å•GPUè®­ç»ƒ
python train.py

# æŒ‡å®šGPU
CUDA_VISIBLE_DEVICES=0 python train.py

# åå°è¿è¡Œï¼ˆæ¨èï¼‰
nohup python train.py > logs/training.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/training.log
```

**æ­¥éª¤3ï¼šç›‘æ§è®­ç»ƒ**

ä½¿ç”¨TensorBoardï¼ˆéœ€è¦åœ¨Trainerä¸­æ·»åŠ SummaryWriterï¼‰ï¼š

```bash
tensorboard --logdir=logs/tensorboard --port=6006
```

### 5.5 é¢„æœŸè®­ç»ƒæ›²çº¿

**Epoch 1-3ï¼ˆå†·å¯åŠ¨é˜¶æ®µï¼‰ï¼š**
```
Epoch 1 Summary:
Train Loss: 3.2456 | Train Acc: 0.4523
Dev Loss: 3.0123 | Dev Acc: 0.4234 | Dev Macro-F1: 0.3987

Epoch 2 Summary:
Train Loss: 2.7834 | Train Acc: 0.5234
Dev Loss: 2.6789 | Dev Acc: 0.5012 | Dev Macro-F1: 0.4756

Epoch 3 Summary:
Train Loss: 2.4567 | Train Acc: 0.5876
Dev Loss: 2.4321 | Dev Acc: 0.5634 | Dev Macro-F1: 0.5423
```

**Epoch 5-10ï¼ˆå¿«é€Ÿæå‡ï¼‰ï¼š**
```
Epoch 5 Summary:
Train Loss: 1.9876 | Train Acc: 0.6543
Dev Loss: 2.1234 | Dev Acc: 0.6234 | Dev Macro-F1: 0.6012

Epoch 8 Summary:
Train Loss: 1.6543 | Train Acc: 0.7234
Dev Loss: 1.9876 | Dev Acc: 0.6756 | Dev Macro-F1: 0.6543
```

**Epoch 13-15ï¼ˆæ”¶æ•›ï¼‰ï¼š**
```
Epoch 15 Summary:
Train Loss: 1.2345 | Train Acc: 0.7856
Dev Loss: 1.7654 | Dev Acc: 0.7123 | Dev Macro-F1: 0.6934
âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: 0.6934)
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] è®­ç»ƒæ­£å¸¸å¯åŠ¨ï¼Œæ— å†…å­˜æº¢å‡º
- [ ] æŸå¤±ç¨³å®šä¸‹é™
- [ ] è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡éƒ½åœ¨æå‡
- [ ] æœ€ä½³æ¨¡å‹æˆåŠŸä¿å­˜
- [ ] Dev F1è¾¾åˆ°66-70%èŒƒå›´

**é¢„è®¡è€—æ—¶ï¼š** 2-3å°æ—¶ï¼ˆ15 epochsï¼Œå•A100 GPUï¼‰

---

## 6. æ¨¡å‹è¯„ä¼°ä¸åˆ†æ

### 6.1 è¯„ä¼°æŒ‡æ ‡è®¡ç®—

**æ¨¡å—è¯´æ˜ï¼š`src/evaluation/metrics.py`**
- **ç”¨é€”**: æä¾›ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—ä¸å±•ç¤ºå·¥å…· `MetricsCalculator`ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€Macro/Weighted F1ã€å„ç±»åˆ« F1ã€æ··æ·†çŸ©é˜µç»˜åˆ¶å’ŒæŠ¥å‘Šæ‰“å°ã€‚
- **è¾“å…¥**: æ¨¡å‹åœ¨è¯„ä¼°é›†ä¸Šçš„é¢„æµ‹ç»“æœ `all_preds`ã€çœŸå®æ ‡ç­¾ `all_labels`ï¼Œä»¥åŠå¯é€‰çš„ç±»åˆ«åå­— `label_names`ã€‚
- **è¾“å‡º**: æŒ‡æ ‡å­—å…¸ `metrics`ï¼ˆåŒ…å«å„ç±» F1 å€¼ã€æ··æ·†çŸ©é˜µå’Œæ–‡æœ¬æŠ¥å‘Šï¼‰ï¼ŒåŒæ—¶å¯å°†æ··æ·†çŸ©é˜µä¿å­˜ä¸ºå›¾ç‰‡å¹¶åœ¨ç»ˆç«¯æ‰“å°è¯¦ç»†æŒ‡æ ‡ä¿¡æ¯ã€‚

```python
# src/evaluation/metrics.py
import torch
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

class MetricsCalculator:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, label_names=['Negative', 'Neutral', 'Positive']):
        self.label_names = label_names
    
    def compute_metrics(self, all_preds, all_labels):
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        # åŸºç¡€æŒ‡æ ‡
        acc = accuracy_score(all_labels, all_preds)
        macro_f1 = f1_score(all_labels, all_preds, average='macro')
        weighted_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        # å„ç±»åˆ«F1
        per_class_f1 = f1_score(all_labels, all_preds, average=None)
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            all_labels,
            all_preds,
            target_names=self.label_names,
            digits=4
        )
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)
        
        metrics = {
            'accuracy': acc,
            'macro_f1': macro_f1,
            'weighted_f1': weighted_f1,
            'per_class_f1': {
                self.label_names[i]: per_class_f1[i]
                for i in range(len(self.label_names))
            },
            'classification_report': report,
            'confusion_matrix': cm
        }
        
        return metrics
    
    def plot_confusion_matrix(self, cm, save_path='results/confusion_matrix.png'):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=self.label_names,
            yticklabels=self.label_names
        )
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300)
        print(f"âœ… æ··æ·†çŸ©é˜µä¿å­˜åˆ°: {save_path}")
    
    def print_metrics(self, metrics):
        """æ‰“å°æŒ‡æ ‡"""
        print("\n" + "="*60)
        print("è¯„ä¼°æŒ‡æ ‡")
        print("="*60)
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"Macro F1: {metrics['macro_f1']:.4f}")
        print(f"Weighted F1: {metrics['weighted_f1']:.4f}")
        print("\nå„ç±»åˆ«F1åˆ†æ•°:")
        for label, f1 in metrics['per_class_f1'].items():
            print(f"  {label}: {f1:.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(metrics['classification_report'])
```

### 6.2 å®Œæ•´è¯„ä¼°è„šæœ¬

**æ¨¡å—è¯´æ˜ï¼š`evaluate.py`**
- **ç”¨é€”**: åŠ è½½è®­ç»ƒå¥½çš„æ£€æŸ¥ç‚¹ï¼Œå¯¹éªŒè¯ / æµ‹è¯•é›†è¿è¡Œå‰å‘æ¨ç†ï¼Œæ”¶é›†é¢„æµ‹ç»“æœï¼Œå¹¶è°ƒç”¨ `MetricsCalculator` å®Œæˆæœ€ç»ˆè¯„ä¼°å’Œå¯é€‰å¯è§†åŒ–ã€‚
- **è¾“å…¥**: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ã€è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼ˆæˆ–é€šè¿‡å†…éƒ¨åˆ›å»ºï¼‰ã€è¿è¡Œè®¾å¤‡ `device`ã€é…ç½®æ–‡ä»¶è·¯å¾„ç­‰ã€‚
- **è¾“å‡º**: æ±‡æ€»åçš„è¯„ä¼°æŒ‡æ ‡ï¼ˆæ‰“å°åˆ°æ§åˆ¶å°æˆ–å†™å…¥æ–‡ä»¶ï¼‰ï¼Œä»¥åŠæ ¹æ®éœ€è¦ä¿å­˜çš„æ··æ·†çŸ©é˜µå›¾ç‰‡æˆ–å…¶ä»–è¯„ä¼°äº§ç‰©ã€‚

```python
# evaluate.py
import torch
import yaml
import os
from tqdm import tqdm
from src.data.create_dataloaders import create_dataloaders
from src.models.agp_model import AGPModel
from src.evaluation.metrics import MetricsCalculator

def load_checkpoint(checkpoint_path, model, device):
    """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"âœ… åŠ è½½æ¨¡å‹: {checkpoint_path}")
    print(f"   Epoch: {checkpoint['epoch']+1}")
    print(f"   Best Dev F1: {checkpoint['best_dev_f1']:.4f}")
    return model

@torch.no_grad()
def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                for k, v in batch.items()}
        
        outputs = model(batch)
        preds = outputs['sentiment_logits'].argmax(dim=1)
        
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())
    
    return all_preds, all_labels

def main():
    # åŠ è½½é…ç½®
    with open('configs/training_config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n1. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    _, _, test_loader, num_aspects = create_dataloaders(
        train_jsonl=config['train_jsonl'],
        dev_jsonl=config['dev_jsonl'],
        test_jsonl=config['test_jsonl'],
        image_root=config['image_root'],
        batch_size=config['batch_size'],
        num_workers=config['num_workers']
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºæ¨¡å‹...")
    model = AGPModel(
        num_aspects=num_aspects,
        hidden_dim=config['hidden_dim'],
        proj_dim=config['proj_dim'],
        num_queries=config['num_queries'],
        num_classes=config['num_classes'],
        freeze_bert_layers=config['freeze_bert_layers'],
        use_lora=config['use_lora'],
        lora_rank=config['lora_rank']
    ).to(device)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    print("\n3. åŠ è½½æ¨¡å‹...")
    checkpoint_path = os.path.join(config['save_dir'], 'best_model.pt')
    model = load_checkpoint(checkpoint_path, model, device)
    
    # è¯„ä¼°
    print("\n4. å¼€å§‹è¯„ä¼°...")
    all_preds, all_labels = evaluate_model(model, test_loader, device)
    
    # è®¡ç®—æŒ‡æ ‡
    print("\n5. è®¡ç®—æŒ‡æ ‡...")
    calculator = MetricsCalculator()
    metrics = calculator.compute_metrics(all_preds, all_labels)
    
    # æ‰“å°æŒ‡æ ‡
    calculator.print_metrics(metrics)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    os.makedirs('results', exist_ok=True)
    calculator.plot_confusion_matrix(
        metrics['confusion_matrix'],
        save_path='results/confusion_matrix.png'
    )
    
    # ä¿å­˜ç»“æœ
    import json
    results_path = 'results/test_results.json'
    with open(results_path, 'w') as f:
        json.dump({
            'accuracy': metrics['accuracy'],
            'macro_f1': metrics['macro_f1'],
            'weighted_f1': metrics['weighted_f1'],
            'per_class_f1': metrics['per_class_f1']
        }, f, indent=2)
    print(f"\nâœ… ç»“æœä¿å­˜åˆ°: {results_path}")

if __name__ == '__main__':
    main()
```

**è¿è¡Œè¯„ä¼°ï¼š**
```bash
python evaluate.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
============================================================
è¯„ä¼°æŒ‡æ ‡
============================================================
Accuracy: 0.7123
Macro F1: 0.6934
Weighted F1: 0.7089

å„ç±»åˆ«F1åˆ†æ•°:
  Negative: 0.6987
  Neutral: 0.6012
  Positive: 0.7803

åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support

    Negative     0.7123    0.6876    0.6987       150
     Neutral     0.6234    0.5812    0.6012        95
    Positive     0.7654    0.7956    0.7803       255

    accuracy                         0.7123       500
   macro avg     0.7004    0.6881    0.6934       500
weighted avg     0.7098    0.7123    0.7089       500
```

### 6.3 é”™è¯¯åˆ†æ

**æ¨¡å—è¯´æ˜ï¼š`src/evaluation/error_analysis.py`**
- **ç”¨é€”**: å¯¹é¢„æµ‹é”™è¯¯çš„æ ·æœ¬è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼ŒåŒ…æ‹¬æŒ‰æ–¹é¢ç»Ÿè®¡ã€æŒ‰çœŸå® / é¢„æµ‹æ ‡ç­¾ç»„åˆç»Ÿè®¡ï¼Œå¹¶å¯¼å‡ºè¯¦ç»†é”™è¯¯æ ·æœ¬è¡¨æ ¼ã€‚
- **è¾“å…¥**: é¢„æµ‹ç»“æœ `all_preds`ã€çœŸå®æ ‡ç­¾ `all_labels`ã€æ ·æœ¬ ID åˆ—è¡¨ `sample_ids`ã€åŸå§‹æ–‡æœ¬ `texts`ã€æ–¹é¢åˆ—è¡¨ `aspects`ã€‚
- **è¾“å‡º**: `pandas.DataFrame` æ ¼å¼çš„é”™è¯¯æ ·æœ¬è¡¨ï¼ˆåŒæ—¶ä¿å­˜ä¸º `results/error_analysis.csv`ï¼‰ï¼Œä»¥åŠåœ¨æ§åˆ¶å°æ‰“å°çš„é”™è¯¯åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯ã€‚

```python
# src/evaluation/error_analysis.py
import pandas as pd

def analyze_errors(all_preds, all_labels, sample_ids, texts, aspects):
    """åˆ†æé¢„æµ‹é”™è¯¯çš„æ ·æœ¬"""
    errors = []
    
    for i, (pred, true) in enumerate(zip(all_preds, all_labels)):
        if pred != true:
            errors.append({
                'sample_id': sample_ids[i],
                'text': texts[i],
                'aspect': aspects[i],
                'true_label': true,
                'pred_label': pred
            })
    
    error_df = pd.DataFrame(errors)
    
    # æŒ‰æ–¹é¢ç»Ÿè®¡é”™è¯¯
    print("\næŒ‰æ–¹é¢ç»Ÿè®¡é”™è¯¯:")
    print(error_df['aspect'].value_counts())
    
    # æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡
    print("\næŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    error_types = error_df.groupby(['true_label', 'pred_label']).size()
    print(error_types)
    
    # ä¿å­˜é”™è¯¯æ ·æœ¬
    error_df.to_csv('results/error_analysis.csv', index=False)
    print("\nâœ… é”™è¯¯åˆ†æä¿å­˜åˆ°: results/error_analysis.csv")
    
    return error_df
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] æµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ°68-72%
- [ ] Macro F1è¾¾åˆ°66-70%
- [ ] å„ç±»åˆ«F1åˆç†ï¼ˆä¸­æ€§ç±»åˆ«é€šå¸¸è¾ƒä½ï¼‰
- [ ] æ··æ·†çŸ©é˜µæ˜¾ç¤ºåˆç†çš„é”™è¯¯åˆ†å¸ƒ

**é¢„è®¡è€—æ—¶ï¼š** 30åˆ†é’Ÿ

---

## 7. è°ƒè¯•ä¸ä¼˜åŒ–

### 7.1 å¸¸è§é—®é¢˜æ’æŸ¥æ¸…å•

#### é—®é¢˜1ï¼šLossä¸ä¸‹é™æˆ–NaN

**å¯èƒ½åŸå› ï¼š**
- å­¦ä¹ ç‡è¿‡å¤§
- æ¢¯åº¦çˆ†ç‚¸
- Pair-ID maské”™è¯¯
- æ‰¹æ¬¡ä¸­æ²¡æœ‰æ­£æ ·æœ¬å¯¹

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# 1. é™ä½å­¦ä¹ ç‡
lr_backbone: 5.0e-6  # ä»1e-5é™åˆ°5e-6
lr_head: 5.0e-5      # ä»1e-4é™åˆ°5e-5

# 2. å¢åŠ æ¢¯åº¦è£å‰ª
max_grad_norm: 0.5   # ä»1.0é™åˆ°0.5

# 3. æ£€æŸ¥pair_id_mask
def verify_pair_id_mask(pair_ids):
    batch_size = len(pair_ids)
    mask = torch.zeros(batch_size, batch_size, dtype=torch.bool)
    for i in range(batch_size):
        for j in range(batch_size):
            if pair_ids[i] == pair_ids[j] and i != j:
                mask[i, j] = True
    # maskåº”è¯¥æ˜¯å¯¹ç§°çš„ä¸”å¯¹è§’çº¿ä¸ºFalse
    assert mask.equal(mask.T), "maskä¸å¯¹ç§°!"
    assert not mask.diagonal().any(), "å¯¹è§’çº¿ä¸åº”ä¸ºTrue!"
    return mask

# 4. å¢å¤§batch sizeä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ­£æ ·æœ¬å¯¹
batch_size: 64  # ä»32å¢åˆ°64
```

#### é—®é¢˜2ï¼šæ˜¾å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# æ–¹æ¡ˆ1ï¼šå‡å°batch size
batch_size: 16

# æ–¹æ¡ˆ2ï¼šæ¢¯åº¦ç´¯ç§¯
accumulation_steps: 2  # æœ‰æ•ˆbatch=16*2=32

# æ–¹æ¡ˆ3ï¼šå‡å°‘æŸ¥è¯¢æ•°é‡
num_queries: 6  # ä»8é™åˆ°6

# æ–¹æ¡ˆ4ï¼šä½¿ç”¨æ›´æ¿€è¿›çš„LoRA
lora_rank: 4  # ä»8é™åˆ°4

# æ–¹æ¡ˆ5ï¼šå†»ç»“æ›´å¤šBERTå±‚
freeze_bert_layers: 11  # ä»10å¢åˆ°11ï¼Œåªå¾®è°ƒæœ€å1å±‚
```

#### é—®é¢˜3ï¼šè¿‡æ‹Ÿåˆï¼ˆTrainé«˜Devä½ï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# 1. å¢å¤§Dropout
# åœ¨æ¨¡å‹ä¸­æ·»åŠ æ›´å¤šdropout
dropout: 0.3  # ä»0.1å¢åˆ°0.3

# 2. å¢å¤§æ¸©åº¦å‚æ•°ï¼ˆé™ä½å¯¹æ¯”å­¦ä¹ çš„ç¡®å®šæ€§ï¼‰
temperature_supcon: 0.2  # ä»0.1å¢åˆ°0.2

# 3. å¢å¤§æƒé‡è¡°å‡
weight_decay: 0.05  # ä»0.01å¢åˆ°0.05

# 4. æ—©åœ
# åœ¨Trainerä¸­æ·»åŠ early stopping
patience: 5  # è¿ç»­5ä¸ªepochéªŒè¯é›†ä¸æå‡åˆ™åœæ­¢

# 5. æ•°æ®å¢å¼º
# å¯¹æ–‡æœ¬è¿›è¡Œéšæœºæ©ç 
# å¯¹å›¾åƒè¿›è¡Œæ›´å¼ºçš„å¢å¼º
```

#### é—®é¢˜4ï¼šæ–¹é¢æ··æ·†ä¸¥é‡

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# 1. å¢å¤§è¾…åŠ©ä»»åŠ¡æƒé‡
gamma: 0.5  # ä»0.3å¢åˆ°0.5

# 2. å¢åŠ æ–¹é¢åŸå‹å¯¹æ¯”å­¦ä¹ ï¼ˆè§CONTRASTIVE_LEARNING_ANALYSIS.mdæ–¹æ¡ˆ1ï¼‰

# 3. å¯è§†åŒ–æ–¹é¢ç‰¹å¾ï¼Œç¡®è®¤æ–¹é¢å¯¼å‘æ€§
```

### 7.2 è¶…å‚æ•°è°ƒä¼˜å»ºè®®

**ä¼˜å…ˆçº§æ’åºï¼š**

1. **å­¦ä¹ ç‡ï¼ˆæœ€é‡è¦ï¼‰**
   ```
   æ¨èèŒƒå›´:
   - lr_backbone: [5e-6, 1e-5, 2e-5]
   - lr_head: [5e-5, 1e-4, 2e-4]
   ```

2. **æŸå¤±æƒé‡**
   ```
   æ¨èèŒƒå›´:
   - alpha (InfoNCE): [0.8, 1.0, 1.5]
   - beta (SupCon): [0.3, 0.5, 0.8]
   - gamma (Aux): [0.2, 0.3, 0.5]
   ```

3. **Batch Size**
   ```
   æ¨è: [16, 32, 64]
   æ³¨æ„ï¼šå¯¹æ¯”å­¦ä¹ å—ç›Šäºå¤§batch
   ```

4. **æ¸©åº¦å‚æ•°**
   ```
   - temperature_infonce: [0.05, 0.07, 0.1]
   - temperature_supcon: [0.07, 0.1, 0.15]
   ```

### 7.3 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# 1. ä½¿ç”¨æ›´å¿«çš„æ•°æ®åŠ è½½
num_workers: 8          # å¢åŠ workeræ•°é‡
pin_memory: True        # ä½¿ç”¨pin memory
persistent_workers: True  # ä¿æŒworkeræŒä¹…åŒ–

# 2. ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model)

# 3. ä½¿ç”¨BF16è€ŒéFP16ï¼ˆA100ï¼‰
# åœ¨Trainerä¸­
with autocast(dtype=torch.bfloat16):
    outputs = model(batch)

# 4. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå‡å°‘æ˜¾å­˜ï¼‰
# åœ¨æ¨¡å‹ä¸­
def forward(self, batch):
    from torch.utils.checkpoint import checkpoint
    Z_text = checkpoint(self.text_cross_attn, queries, text_features, text_features)
```

### 7.4 æ¶ˆèå®éªŒè®¾è®¡

**ç›®çš„ï¼šéªŒè¯å„ç»„ä»¶çš„è´¡çŒ®**

```python
# å®éªŒ1ï¼šBaselineï¼ˆæ— LLMæ‰©å†™ï¼‰
aspect_desc = sample['aspect']  # ä¸ä½¿ç”¨LLMæ‰©å†™

# å®éªŒ2ï¼šæ— Aspect-Aware SupCon
# ä½¿ç”¨åŸå§‹SupConï¼ˆä¸è€ƒè™‘æ–¹é¢ï¼‰
pos_mask = (label_match).float()  # ç§»é™¤aspect_match

# å®éªŒ3ï¼šæ— è¾…åŠ©ä»»åŠ¡
gamma: 0.0  # å…³é—­è¾…åŠ©ä»»åŠ¡

# å®éªŒ4ï¼šæ— Attention Pooling
# ä½¿ç”¨MeanPoolingæ›¿ä»£AttentionPooling

# å®éªŒ5ï¼šå‡å°‘æŸ¥è¯¢æ•°é‡
num_queries: 4  # ä»8é™åˆ°4
```

**âœ… æ£€æŸ¥ç‚¹ï¼š**
- [ ] äº†è§£å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- [ ] æŒæ¡è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥
- [ ] çŸ¥é“å¦‚ä½•è®¾è®¡æ¶ˆèå®éªŒ
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½ä¼˜åŒ–

**é¢„è®¡è€—æ—¶ï¼š** æ ¹æ®å…·ä½“é—®é¢˜è€Œå®š

---

## é™„å½•

### A. å®Œæ•´æ–‡ä»¶æ¸…å•

```
AGP-MABSA/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ dev.jsonl
â”‚   â”‚   â””â”€â”€ test.jsonl
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ train_expanded.jsonl
â”‚   â”‚   â”œâ”€â”€ dev_expanded.jsonl
â”‚   â”‚   â””â”€â”€ test_expanded.jsonl
â”‚   â””â”€â”€ images/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ create_dataloaders.py
â”‚   â”‚   â””â”€â”€ llm_expansion.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoders.py
â”‚   â”‚   â”œâ”€â”€ query_generator.py
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â”œâ”€â”€ pooling.py
â”‚   â”‚   â”œâ”€â”€ projector.py
â”‚   â”‚   â””â”€â”€ agp_model.py
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ classification.py
â”‚   â”‚   â”œâ”€â”€ infonce.py
â”‚   â”‚   â”œâ”€â”€ supcon.py
â”‚   â”‚   â”œâ”€â”€ auxiliary.py
â”‚   â”‚   â””â”€â”€ total_loss.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ error_analysis.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### B. å¿«é€Ÿå¼€å§‹å‘½ä»¤

```bash
# 1. ç¯å¢ƒé…ç½®ï¼ˆ30åˆ†é’Ÿï¼‰
conda create -n agp_mabsa python=3.9
conda activate agp_mabsa
pip install -r requirements.txt

# 2. æ•°æ®é¢„å¤„ç†ï¼ˆ2å°æ—¶ï¼‰
python src/data/llm_expansion.py

# 3. æµ‹è¯•æ¨¡å‹ï¼ˆ10åˆ†é’Ÿï¼‰
cd src/models
python agp_model.py

# 4. è®­ç»ƒæ¨¡å‹ï¼ˆ2-3å°æ—¶ï¼‰
python train.py

# 5. è¯„ä¼°æ¨¡å‹ï¼ˆ10åˆ†é’Ÿï¼‰
python evaluate.py
```

### C. é¢„æœŸæ—¶é—´çº¿

| æ­¥éª¤ | é¢„è®¡è€—æ—¶ | ç´¯è®¡è€—æ—¶ |
|------|---------|---------|
| 1. ç¯å¢ƒå‡†å¤‡ | 30åˆ†é’Ÿ | 0.5å°æ—¶ |
| 2. æ•°æ®é¢„å¤„ç† | 2å°æ—¶ | 2.5å°æ—¶ |
| 3. æ¨¡å‹å®ç° | 4-6å°æ—¶ | 8.5å°æ—¶ |
| 4. æŸå¤±å‡½æ•° | 2-3å°æ—¶ | 11å°æ—¶ |
| 5. è®­ç»ƒæ‰§è¡Œ | 2-3å°æ—¶ | 14å°æ—¶ |
| 6. æ¨¡å‹è¯„ä¼° | 1å°æ—¶ | 15å°æ—¶ |
| **æ€»è®¡** | **12-15å°æ—¶** | |

### D. è”ç³»ä¸æ”¯æŒ

é‡åˆ°é—®é¢˜æ—¶ï¼š
1. æ£€æŸ¥æœ¬æ–‡æ¡£çš„"è°ƒè¯•ä¸ä¼˜åŒ–"ç« èŠ‚
2. æŸ¥é˜…`CONTRASTIVE_LEARNING_ANALYSIS.md`çš„æ”¹è¿›å»ºè®®
3. å‚è€ƒ`AGAA METHOD GUIDE.md`çš„å®ç°ç»†èŠ‚
4. æŸ¥çœ‹GitHub Issuesï¼ˆå¦‚æœ‰ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬:** 1.0  
**æœ€åæ›´æ–°:** 2026-01-27  
**æ–‡æ¡£çŠ¶æ€:** å·²å®Œæˆ  
**æ€»å­—æ•°:** ~15,000å­—

ğŸ‰ **æ­å–œï¼æ‚¨å·²å®ŒæˆAGPå®éªŒæ­¥éª¤æ–‡æ¡£çš„å­¦ä¹ ã€‚ç°åœ¨å¯ä»¥å¼€å§‹å®éªŒäº†ï¼**
